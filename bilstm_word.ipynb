{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/HarmanDotpy/Named-Entity-Recognition-in-Pytorch/blob/main/scripts/train_bilstm_char_random_glove.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import io\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle as pickle\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import seqeval\n",
    "from seqeval.metrics import accuracy_score as seq_accuracy_score\n",
    "from seqeval.metrics import classification_report as seq_classification_report\n",
    "from seqeval.metrics import f1_score as seq_f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BILSTM model\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, total_words, num_class, pretrained = False, pretrained_embed = None):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wordembed = nn.Embedding.from_pretrained(pretrained_embed, freeze = False)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.bilstm = nn.LSTM(embedding_size,hidden_size, bidirectional = True, batch_first = True)\n",
    "        self.linear = nn.Linear(2*hidden_size, num_class) # 2 because forward and backward concatenate\n",
    "\n",
    "    def forward(self, x, xlengths): \n",
    "        x = pack_padded_sequence(x, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x, _ = pad_packed_sequence(x, batch_first=True)\n",
    "        word_embedding = self.wordembed(x) # x is of size(batchsize, seq_len), out is of size (batchsize, seq_len, embedding_size = 100)\n",
    "        # word_embedding = self.fcembed(word_embedding)\n",
    "        word_embedding = self.dropout(word_embedding) # dropout\n",
    "\n",
    "        out, (h,c) = self.bilstm(word_embedding) #'out' has dimension(batchsize, seq_len, 2*hidden_size)\n",
    "        out = self.linear(out) # now 'out' has dimension(batchsize, seq_len, num_class)\n",
    "        out = out.view(-1, out.shape[2]) # shape (128*seqlen, 18)\n",
    "        out = F.log_softmax(out, dim=1) # take the softmax across the dimension num_class, 'out' has dimension(batchsize, seq_len, num_class)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading text file in python and making list of sentences (list of lists) and list of tags(list of lists)\n",
    "def load_data(datapath, buildvocab_tags= True, vocab = None, nertags = None):\n",
    "    if(buildvocab_tags == True):\n",
    "        all_words = []\n",
    "        all_tags = []\n",
    "        with open(datapath) as f:\n",
    "            lines = f.readlines()\n",
    "            sent_num = 0\n",
    "            for line in lines[2:]: #1: so that the first blank line isn't taken into account\n",
    "                if(line == \"\\n\"):\n",
    "                    sent_num+=1\n",
    "                else:\n",
    "                    line_sep = line.split(sep = \" \")\n",
    "                    all_words.append(line_sep[0])\n",
    "                    all_tags.append(line_sep[3][:-1])\n",
    "                    \n",
    "        words = list(set(all_words))\n",
    "        tags = list(set(all_tags))\n",
    "\n",
    "        vocab = {}\n",
    "        vocab['<pad>'] = 0 # for padding input sequences\n",
    "        vocab['<oov>'] = 1\n",
    "        for i, word in enumerate(words):\n",
    "            vocab[word] = i+2\n",
    "            \n",
    "        nertags = {}\n",
    "        nertags['padtag'] = 0\n",
    "        for i,nertag in enumerate(tags):\n",
    "            nertags[nertag] = i+1\n",
    "\n",
    "    train_sent = []\n",
    "    train_tags = []\n",
    "    with open(datapath) as f:\n",
    "        lines = f.readlines()\n",
    "        sent_num = 0\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        for line in lines[2:]: #1: so that the first blank line isn't taken into account\n",
    "            if(line == \"\\n\"):\n",
    "                sent_num+=1\n",
    "                train_sent.append(sentence)\n",
    "                train_tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                line_sep = line.split(sep = \" \")\n",
    "                if(line_sep[0] in vocab.keys()):\n",
    "                    sentence.append(vocab[line_sep[0]])\n",
    "                else:\n",
    "                    sentence.append(vocab['<oov>'])\n",
    "                    \n",
    "                tag.append(nertags[line_sep[3][:-1]])\n",
    "\n",
    "    # padding the sentences at the end\n",
    "    seq_maxlen = max(len(x) for x in train_sent)\n",
    "    x_lengths = [len(x) for x in train_sent]\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    for sent, tags in zip(train_sent, train_tags):\n",
    "        length_toappend = seq_maxlen - len(sent)\n",
    "        Xtrain.append(sent+[0]*length_toappend)\n",
    "        Ytrain.append(tags+[0]*length_toappend)\n",
    "\n",
    "\n",
    "    Xtrain = torch.Tensor(Xtrain)\n",
    "    Ytrain = torch.Tensor(Ytrain)\n",
    "    x_lengths = torch.Tensor(x_lengths)\n",
    "    print(Xtrain.shape, Ytrain.shape, x_lengths.shape)\n",
    "    \n",
    "    return Xtrain, Ytrain, x_lengths, vocab, nertags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14986, 113]) torch.Size([14986, 113]) torch.Size([14986])\n",
      "torch.Size([3465, 109]) torch.Size([3465, 109]) torch.Size([3465])\n"
     ]
    }
   ],
   "source": [
    "traindatapath = 'data/train.txt'\n",
    "devdatapath = 'data/dev.txt'\n",
    "testdatapath = 'data/test.txt'\n",
    "\n",
    "\n",
    "Xtrain, Ytrain, x_trainlengths, vocab, nertags = load_data(traindatapath, buildvocab_tags=True)\n",
    "Xdev, Ydev, x_devlengths, _, _ = load_data(devdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = TensorDataset(Xtrain, Ytrain, x_trainlengths)\n",
    "Trainloader = DataLoader(traindataset, batch_size= 128, shuffle=True)\n",
    "\n",
    "devdataset = TensorDataset(Xdev, Ydev, x_devlengths)\n",
    "Devloader = DataLoader(devdataset, batch_size= 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MY MODEL!!! \n",
    "\n",
    "pre_embeddings = 'glove'\n",
    "Expname = 'BILSTM_glove'\n",
    "rootpath = 'out/'\n",
    "glove_embeddings_file = 'data/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EMBEDDINGS\n",
    "embedding_size = 100\n",
    "if(pre_embeddings == \"glove\"):\n",
    "    gloveembeddings_index = {}\n",
    "    with io.open(glove_embeddings_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:],dtype='float32')\n",
    "            gloveembeddings_index[word] = coefs\n",
    "\n",
    "    #using vocab and Xtrain, Xvalid, get pretrained glove word embeddings\n",
    "    glove_embeds = np.zeros((len(vocab), embedding_size))\n",
    "    for word in vocab.keys():\n",
    "        if(word in gloveembeddings_index.keys()):\n",
    "            # for the pad word let theembedding be all zeros\n",
    "            glove_embeds[vocab[word]] = gloveembeddings_index[word]\n",
    "        else:\n",
    "            glove_embeds[vocab[word]] = np.random.randn(embedding_size)\n",
    "    word_embeds = torch.Tensor(glove_embeds)\n",
    "    # print(glove_embeds.shape) # shape (vocablength , embedding dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes to be looked at for performance metrics\n",
    "imp_classes = [nertags[tag] for tag in nertags.keys()]\n",
    "imp_classes.remove(nertags['padtag'])\n",
    "imp_classes.remove(nertags['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(embedding_size = 100, hidden_size = 100, total_words = len(vocab), num_class = 18, pretrained = True, pretrained_embed = word_embeds).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "lossfunction = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (wordembed): Embedding(23626, 100)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (bilstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=200, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(y, ypred, nertags):\n",
    "    y = y.numpy()\n",
    "    ypred = ypred.numpy()\n",
    "    mask = (y != nertags['padtag']) * (y != nertags['O'])\n",
    "    y = y*mask\n",
    "    ypred = ypred*mask\n",
    "    acc = ((y==ypred)*mask).sum()/mask.sum()\n",
    "    microf1 = f1_score(y, ypred, labels = imp_classes, average='micro')\n",
    "    macrof1 = f1_score(y, ypred, labels = imp_classes, average='macro')\n",
    "\n",
    "    return acc, microf1, macrof1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "        with torch.no_grad():\n",
    "            validloss = 0\n",
    "            acc = 0\n",
    "            microf1 = 0\n",
    "            macrof1 = 0\n",
    "            i = 0\n",
    "            for step, (X, Y, xlen) in enumerate(loader):\n",
    "                Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
    "                Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
    "                ypred = model(X.long().to(device), xlen.to(device))#.permute(0, 2, 1)\n",
    "                vloss = lossfunction(ypred.to('cpu'), Y.view(-1).type(torch.LongTensor))\n",
    "                validloss+=vloss.item()\n",
    "                acc_, microf1_, macrof1_ = performance(Y.view(-1), torch.argmax(ypred.to('cpu'), dim = 1), nertags)\n",
    "                acc+=acc_\n",
    "                microf1 += microf1_\n",
    "                macrof1 += macrof1_\n",
    "                i+=1\n",
    "\n",
    "        return validloss/i, acc/i, microf1/i, macrof1/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlosslist = []\n",
    "trainacclist = []\n",
    "trainmicrof1list = []\n",
    "trainmacrof1list = []\n",
    "\n",
    "validlosslist = []\n",
    "valacclist = []\n",
    "valmicrof1list = []\n",
    "valmacrof1list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.00592148047738329, microF1 = 0.009905183453201459, macroF1 = 0.0059333926121512975\n",
      "training accuracy = 0.031061360804754686, microF1 = 0.05170414976523872, macroF1 = 0.030814309867076742\n",
      "training accuracy = 0.07403233059838846, microF1 = 0.1084353683846174, macroF1 = 0.06417055159781623\n",
      "training accuracy = 0.1314899390634953, microF1 = 0.1758374571935371, macroF1 = 0.1136655419465107\n",
      "training accuracy = 0.18568462957041693, microF1 = 0.23491640567471206, macroF1 = 0.16108829699616684\n",
      "\n",
      "epoch = 0, training_loss = 0.16486718344612647, validation_loss =0.09755267123026508, training_acc = 0.2268808170333198, validation_acc =0.5152687180299405\n",
      "training accuracy = 0.5576532439708274, microF1 = 0.605211855777909, macroF1 = 0.5127872326453906\n",
      "training accuracy = 0.5788517542958617, microF1 = 0.6252319589768702, macroF1 = 0.5439283952944276\n",
      "training accuracy = 0.5967323480275121, microF1 = 0.6433319407712148, macroF1 = 0.5696786384315435\n",
      "training accuracy = 0.6165042164376768, microF1 = 0.6625468094811527, macroF1 = 0.594331406205617\n",
      "training accuracy = 0.6294890065069259, microF1 = 0.6740542977939658, macroF1 = 0.6078023722869544\n",
      "\n",
      "epoch = 1, training_loss = 0.06072408071387622, validation_loss =0.08490795815097434, training_acc = 0.6382499695061998, validation_acc =0.6944578653664585\n",
      "training accuracy = 0.7338707307529028, microF1 = 0.7641068406552896, macroF1 = 0.7203087107195175\n",
      "training accuracy = 0.7523738186635646, microF1 = 0.7822226203081468, macroF1 = 0.7432408585440956\n",
      "training accuracy = 0.7652176878790204, microF1 = 0.7949881192505707, macroF1 = 0.7626111244201009\n",
      "training accuracy = 0.7747091686722153, microF1 = 0.8036811777829983, macroF1 = 0.7717860213531227\n",
      "training accuracy = 0.7790424080894115, microF1 = 0.807931091858262, macroF1 = 0.7747348480793931\n",
      "\n",
      "epoch = 2, training_loss = 0.038737414665204487, validation_loss =0.07772700568395001, training_acc = 0.7822790765601038, validation_acc =0.7662003459844345\n",
      "training accuracy = 0.8278401573508554, microF1 = 0.8490836216339158, macroF1 = 0.8133707315722046\n",
      "training accuracy = 0.8388177896916623, microF1 = 0.8595441010902595, macroF1 = 0.8324855014804661\n",
      "training accuracy = 0.8410719580123323, microF1 = 0.8621492172805346, macroF1 = 0.8329795934843712\n",
      "training accuracy = 0.8475912901251048, microF1 = 0.8685968516598535, macroF1 = 0.8422463937176747\n",
      "training accuracy = 0.8520315056334039, microF1 = 0.8726661710999554, macroF1 = 0.8478114499676983\n",
      "\n",
      "epoch = 3, training_loss = 0.02641209365673742, validation_loss =0.07806020895285266, training_acc = 0.8531443607514503, validation_acc =0.7997674459299071\n",
      "training accuracy = 0.8976287959298241, microF1 = 0.9121258264363566, macroF1 = 0.8971751953115656\n",
      "training accuracy = 0.8990050823648177, microF1 = 0.9140888513519031, macroF1 = 0.8922353454863592\n",
      "training accuracy = 0.9009038569857214, microF1 = 0.915800967831728, macroF1 = 0.8933271419329769\n",
      "training accuracy = 0.9004770991332631, microF1 = 0.9154141317879831, macroF1 = 0.8951831246352252\n",
      "training accuracy = 0.9006844132941659, microF1 = 0.915587933731006, macroF1 = 0.8960393714664278\n",
      "\n",
      "epoch = 4, training_loss = 0.01796827852031437, validation_loss =0.08428817096033267, training_acc = 0.901686105096257, validation_acc =0.8097371463874267\n",
      "training accuracy = 0.924414268503433, microF1 = 0.9360464489174896, macroF1 = 0.9219051321635761\n",
      "training accuracy = 0.9242702723771167, microF1 = 0.9355520489298622, macroF1 = 0.9196935276744925\n",
      "training accuracy = 0.9237309340759817, microF1 = 0.9356084760898958, macroF1 = 0.9226926732652274\n",
      "training accuracy = 0.9239023500008999, microF1 = 0.9363076749505918, macroF1 = 0.922887968670295\n",
      "training accuracy = 0.9249336496639748, microF1 = 0.9371243481339948, macroF1 = 0.9240718399463712\n",
      "\n",
      "epoch = 5, training_loss = 0.013595162470572455, validation_loss =0.08034855260380677, training_acc = 0.9260122061061252, validation_acc =0.8126171024506949\n",
      "training accuracy = 0.922395030704942, microF1 = 0.9347020229552785, macroF1 = 0.9197888510583816\n",
      "training accuracy = 0.9260028074971691, microF1 = 0.9374688617382745, macroF1 = 0.9231861867793606\n",
      "training accuracy = 0.9298018058432551, microF1 = 0.9411825756420239, macroF1 = 0.9262160927829409\n",
      "training accuracy = 0.9290719830342283, microF1 = 0.9405867712803299, macroF1 = 0.9267284348851536\n",
      "training accuracy = 0.9305109764080399, microF1 = 0.9416867730322589, macroF1 = 0.9285166122958157\n",
      "\n",
      "epoch = 6, training_loss = 0.013250565829181696, validation_loss =0.08010830224624702, training_acc = 0.9300492902096166, validation_acc =0.8177091263755846\n",
      "training accuracy = 0.9368963430622221, microF1 = 0.947715781351729, macroF1 = 0.9308088746484312\n",
      "training accuracy = 0.9325891320791606, microF1 = 0.9433283301218556, macroF1 = 0.9250437777961317\n",
      "training accuracy = 0.9313952091845088, microF1 = 0.9426292249369655, macroF1 = 0.9254408034407416\n",
      "training accuracy = 0.9307828736096675, microF1 = 0.9421335062076044, macroF1 = 0.9260666198517379\n",
      "training accuracy = 0.9317895244938166, microF1 = 0.9431888440575208, macroF1 = 0.9279749533306636\n",
      "\n",
      "epoch = 7, training_loss = 0.012827705228859084, validation_loss =0.08232346456497908, training_acc = 0.9306174333080656, validation_acc =0.8180603841424453\n",
      "training accuracy = 0.9296556003961773, microF1 = 0.9424582068283671, macroF1 = 0.9278308380780838\n",
      "training accuracy = 0.931578881082183, microF1 = 0.9445039836993437, macroF1 = 0.9279751444210683\n",
      "training accuracy = 0.9298729591262693, microF1 = 0.9422116859779567, macroF1 = 0.9263171351099455\n",
      "training accuracy = 0.9295662601851752, microF1 = 0.9419785319053104, macroF1 = 0.9260520257648923\n",
      "training accuracy = 0.9300933985082663, microF1 = 0.9420761414137883, macroF1 = 0.9254615550262804\n",
      "\n",
      "epoch = 8, training_loss = 0.01243506714512231, validation_loss =0.07984749866383416, training_acc = 0.9314617258961244, validation_acc =0.8231513007375251\n",
      "training accuracy = 0.9345854030273625, microF1 = 0.9445770283372377, macroF1 = 0.9354276345170478\n",
      "training accuracy = 0.9362881778795626, microF1 = 0.9462180411964537, macroF1 = 0.9351347886178936\n",
      "training accuracy = 0.9327382358427043, microF1 = 0.9435222451591083, macroF1 = 0.9304814447069562\n",
      "training accuracy = 0.9321114635144273, microF1 = 0.9434118344514975, macroF1 = 0.9302145501523867\n",
      "training accuracy = 0.9308387019557264, microF1 = 0.9423299321495623, macroF1 = 0.9282921120744619\n",
      "\n",
      "epoch = 9, training_loss = 0.012757625237454549, validation_loss =0.08526827142174755, training_acc = 0.9312505914656422, validation_acc =0.8139059335301487\n"
     ]
    }
   ],
   "source": [
    "# Model is ready now we have to train using cross entropy loss\n",
    "num_epochs = 10\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# validloss = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    if(epoch == 5):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "        \n",
    "    totalloss, acc, microf1, macrof1 = 0, 0, 0, 0\n",
    "    for step, (Xbatch ,Ybatch, xbatch_len) in enumerate(Trainloader):\n",
    "        #make gradients 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Ybatch = pack_padded_sequence(Ybatch, xbatch_len, batch_first=True, enforce_sorted=False)\n",
    "        Ybatch, y_lengths = pad_packed_sequence(Ybatch, batch_first=True)\n",
    "\n",
    "        #get output from model and claculate loss\n",
    "        ypred = model(Xbatch.long().to(device), xbatch_len.to(device))#.permute(0, 2, 1)\n",
    "        \n",
    "        acc_, microf1_, macrof1_ = performance(Ybatch.view(-1), torch.argmax(ypred.to('cpu'), dim = 1), nertags)\n",
    "        acc+= acc_\n",
    "        microf1+=microf1_\n",
    "        macrof1+=macrof1_\n",
    "        if(step%20 == 0 and step !=0):\n",
    "            print(\"training accuracy = {}, microF1 = {}, macroF1 = {}\".format(acc/(step+1), microf1/(step+1), macrof1/(step+1)))\n",
    "            \n",
    "        loss = lossfunction(ypred.to('cpu'), Ybatch.view(-1).type(torch.LongTensor)) #Ybatch has dimension (batchsize, seqlen), ypred has dimension(batchsize, num_classes, seqlen)\n",
    "        totalloss += loss.item()\n",
    "\n",
    "        #backward and step\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5) # clip gradient to 5\n",
    "        optimizer.step()\n",
    "        \n",
    "    trainlosslist.append(totalloss/(step+1))\n",
    "    trainacclist.append(acc/(step+1))\n",
    "    trainmicrof1list.append(microf1/(step+1))\n",
    "    trainmacrof1list.append(macrof1/(step+1))\n",
    "\n",
    "    # model validation loss and scheduler step for learning rate change if required\n",
    "    val_loss, val_acc, val_microf1, val_macrof1  = validate(model, Devloader)\n",
    "    validlosslist.append(val_loss)\n",
    "    valacclist.append(val_acc)\n",
    "    valmicrof1list.append(val_microf1)\n",
    "    valmacrof1list.append(val_macrof1)\n",
    "        \n",
    "    # scheduler.step(val_loss)\n",
    "    print('\\nepoch = {}, training_loss = {}, validation_loss ={}, training_acc = {}, validation_acc ={}'.format(epoch, trainlosslist[-1], validlosslist[-1], trainacclist[-1], valacclist[-1]))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {id: word for word, id in vocab.items()} \n",
    "id2tag = {}\n",
    "for tag in nertags.keys():\n",
    "    if(tag == 'padtag'):\n",
    "         id2tag[nertags[tag]] = 'O' \n",
    "    else:\n",
    "        id2tag[nertags[tag]] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'out/bilstm_word.pt' )\n",
    "with open('out/id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)\n",
    "with open('out/id2tag.pkl', 'wb') as f:\n",
    "    pickle.dump(id2tag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state dictionary\n",
    "torch.save(model.state_dict(), 'models/trained_bilstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state dictionary\n",
    "torch.save(model.state_dict(), 'models/bilstm_word/model.pth')\n",
    "\n",
    "# Save the dictionaries\n",
    "with open('models/bilstm_word/id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)\n",
    "with open('models/bilstm_word/id2tag.pkl', 'wb') as f:\n",
    "    pickle.dump(id2tag, f)\n",
    "with open('models/bilstm_word/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "with open('models/bilstm_word/nertags.pkl', 'wb') as f:\n",
    "    pickle.dump(nertags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (wordembed): Embedding(23626, 100)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bilstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=200, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(rootpath):\n",
    "       os.mkdir(rootpath)\n",
    "\n",
    "if not os.path.exists(rootpath+Expname):\n",
    "    os.mkdir(rootpath+Expname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_metrics(model,loader):\n",
    "    y_predicted = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for step, (X, Y, xlen) in enumerate(loader):\n",
    "            Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
    "            Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
    "            ypred = model(X.long().to(device), xlen.to(device))#.permute(0, 2, 1)\n",
    "            ypred = torch.argmax(ypred.to('cpu'), dim = 1)\n",
    "            ypred = ypred.view(Y.shape[0], -1)\n",
    "            y_predicted.append(ypred)\n",
    "            y_true.append(Y)\n",
    "\n",
    "    y_predicted_list = []\n",
    "    y_true_list = []\n",
    "    for i in range(len(y_predicted)):\n",
    "        for j in range(y_predicted[i].shape[0]):\n",
    "            sent_pred = []\n",
    "            sent_true = []\n",
    "            for x in range(y_predicted[i].shape[1]):\n",
    "                sent_pred.append(id2tag[int(y_predicted[i][j, x])])\n",
    "                sent_true.append(id2tag[int(y_true[i][j, x])])\n",
    "            y_predicted_list.append(sent_pred)\n",
    "            y_true_list.append(sent_true)\n",
    "    \n",
    "    return seq_f1_score(y_true_list, y_predicted_list), seq_accuracy_score(y_true_list, y_predicted_list), seq_classification_report(y_true_list, y_predicted_list, digits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3683, 124]) torch.Size([3683, 124]) torch.Size([3683])\n",
      "PERFORMANCE ON Test DATA\n",
      "MicroF1 = 0.583196516525091\n",
      "Accuracy = 0.8931318975677702\n",
      "------------Classification Report-------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC      0.666     0.731     0.697      1668\n",
      "        MISC      0.192     0.764     0.307       702\n",
      "         ORG      0.784     0.613     0.688      1661\n",
      "         PER      0.538     0.811     0.646      1617\n",
      "\n",
      "   micro avg      0.489     0.723     0.583      5648\n",
      "   macro avg      0.545     0.730     0.585      5648\n",
      "weighted avg      0.605     0.723     0.631      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test DATASET\n",
    "Xtest, Ytest, x_testlengths, _, _ = load_data(testdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)\n",
    "\n",
    "testdataset = TensorDataset(Xtest, Ytest, x_testlengths)\n",
    "loader_test = DataLoader(testdataset, batch_size= 1, shuffle=False)\n",
    "test_f1_conll, test_acc_conll, test_classif_report = final_metrics(model, loader_test)\n",
    "\n",
    "print(\"PERFORMANCE ON Test DATA\")\n",
    "print('MicroF1 = {}'.format(test_f1_conll))\n",
    "print('Accuracy = {}'.format(test_acc_conll))\n",
    "print('------------Classification Report-------------')\n",
    "print(test_classif_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_predictions(model, loader, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        with torch.no_grad():\n",
    "            for step, (X, Y, xlen) in enumerate(loader):\n",
    "                Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
    "                Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
    "                ypred = model(X.long().to(device), xlen.to(device))\n",
    "                ypred = torch.argmax(ypred.to('cpu'), dim=1)\n",
    "                ypred = ypred.view(Y.shape[0], -1)\n",
    "                for i in range(len(ypred)):\n",
    "                    for j in range(len(ypred[i])):\n",
    "                        word = id2word[int(X[i, j])]\n",
    "                        tag = id2tag[int(ypred[i, j])]\n",
    "                        f.write(f\"{word}\\t{tag}\\n\")\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_predictions(model, loader_test, 'predictions/gold_bilstm_word.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete????\n",
    "\n",
    "# Open the input and output files\n",
    "with open('data/test.txt', 'r') as f_input, open('data/gold.txt', 'w') as f_output:\n",
    "    # Read each line from the input file\n",
    "    for line_number, line in enumerate(f_input):\n",
    "        if line_number < 2:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            f_output.write('\\n')\n",
    "            continue  \n",
    "        columns = line.split()\n",
    "        if len(columns) < 2:\n",
    "            continue  \n",
    "        word = columns[0]\n",
    "        ner_tag = columns[-1]\n",
    "        f_output.write(word + '\\t' + ner_tag + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBIO(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[1])\n",
    "    return ents\n",
    "\n",
    "def toSpans(tags):\n",
    "    spans = set()\n",
    "    for beg in range(len(tags)):\n",
    "        if tags[beg][0] == 'B':\n",
    "            end = beg\n",
    "            for end in range(beg+1, len(tags)):\n",
    "                if tags[end][0] != 'I':\n",
    "                    break\n",
    "            spans.add(str(beg) + '-' + str(end) + ':' + tags[beg][2:])\n",
    "            #print(end-beg)\n",
    "    return spans\n",
    "\n",
    "def getInstanceScores(predPath, goldPath):\n",
    "    goldEnts = readBIO(goldPath)\n",
    "    predEnts = readBIO(predPath)\n",
    "    entScores = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for goldEnt, predEnt in zip(goldEnts, predEnts):\n",
    "        goldSpans = toSpans(goldEnt)\n",
    "        predSpans = toSpans(predEnt)\n",
    "        overlap = len(goldSpans.intersection(predSpans))\n",
    "        tp += overlap\n",
    "        fp += len(predSpans) - overlap\n",
    "        fn += len(goldSpans) - overlap\n",
    "        \n",
    "    prec = 0.0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    rec = 0.0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0.0 if prec+rec == 0.0 else 2 * (prec * rec) / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 'predictions/gold_bilstm_word.txt'\n",
    "gold = 'data/gold.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-F1 score - word-biLSTM  0.6228358208955224\n"
     ]
    }
   ],
   "source": [
    "score = getInstanceScores(pred,gold)\n",
    "print('Span-F1 score - word-biLSTM ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datapath):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(datapath) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        for line in lines:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if line:  # If the line is not empty\n",
    "                word, tag_label = line.split('\\t')\n",
    "                if vocab is not None:\n",
    "                    if word in vocab.keys():\n",
    "                        sentence.append(vocab[word])\n",
    "                    else:\n",
    "                        sentence.append(vocab['<oov>'])\n",
    "                if nertags is not None:\n",
    "                    tag.append(nertags[tag_label])\n",
    "            else:  # If the line is empty, indicating end of a sentence\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    tags.append(tag)\n",
    "                    sentence = []\n",
    "                    tag = []\n",
    "\n",
    "    # Padding the sentences at the end\n",
    "    max_length = max(len(x) for x in sentences)\n",
    "    x_lengths = [len(x) for x in sentences]\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    for sent, tag in zip(sentences, tags):\n",
    "        length_to_append = max_length - len(sent)\n",
    "        X_test.append(sent + [0] * length_to_append)  # Padding with zeros\n",
    "        Y_test.append(tag + [0] * length_to_append)  # Padding with zeros\n",
    "\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    Y_test = torch.Tensor(Y_test)\n",
    "    x_lengths = torch.Tensor(x_lengths)\n",
    "\n",
    "    return X_test, Y_test, x_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rates = {\n",
    "    'capitalization_swap': [0.1, 0.15, 0.2, 0.25, 0.3], \n",
    "    'character_swap': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_removal': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_replacement': [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_test(testdatapath):\n",
    "    Xtest, Ytest, x_testlengths = load_data(testdatapath)\n",
    "\n",
    "    testdataset = TensorDataset(Xtest, Ytest, x_testlengths)\n",
    "    loader_test = DataLoader(testdataset, batch_size=1, shuffle=False)\n",
    "    return loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/florentinafabregas/Documents/ITU/ultimate NLP/NLP/bilstm_word.ipynb Cell 38\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m rate \u001b[39min\u001b[39;00m noise_rates[noise_type]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     my_loader \u001b[39m=\u001b[39m loader_test(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/altered/\u001b[39m\u001b[39m{\u001b[39;00mnoise_type\u001b[39m}\u001b[39;00m\u001b[39m_rate_\u001b[39m\u001b[39m{\u001b[39;00mrate\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     out_predictions(model, my_loader, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpredictions/word/\u001b[39m\u001b[39m{\u001b[39;00mnoise_type\u001b[39m}\u001b[39;00m\u001b[39m_rate_\u001b[39m\u001b[39m{\u001b[39;00mrate\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     score \u001b[39m=\u001b[39m getInstanceScores(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpredictions/word/\u001b[39m\u001b[39m{\u001b[39;00mnoise_type\u001b[39m}\u001b[39;00m\u001b[39m_rate_\u001b[39m\u001b[39m{\u001b[39;00mrate\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpredictions/gold_bilstm_word.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Append data to lists\u001b[39;00m\n",
      "\u001b[1;32m/Users/florentinafabregas/Documents/ITU/ultimate NLP/NLP/bilstm_word.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(ypred[i])):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     word \u001b[39m=\u001b[39m id2word[\u001b[39mint\u001b[39m(X[i, j])]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     tag \u001b[39m=\u001b[39m id2tag[\u001b[39mint\u001b[39m(ypred[i, j])]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mtag\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/florentinafabregas/Documents/ITU/ultimate%20NLP/NLP/bilstm_word.ipynb#X55sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "noise_types = []\n",
    "noise_rate = []\n",
    "f1_scores = []\n",
    "\n",
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        my_loader = loader_test(f'data/altered/{noise_type}_rate_{rate}.txt')\n",
    "        out_predictions(model, my_loader, f'predictions/word/{noise_type}_rate_{rate}.txt')\n",
    "        score = getInstanceScores(f'predictions/word/{noise_type}_rate_{rate}.txt', 'predictions/gold_bilstm_word.txt')\n",
    "        \n",
    "        # Append data to lists\n",
    "        noise_types.append(noise_type)\n",
    "        noise_rate.append(rate)\n",
    "        f1_scores.append(score)\n",
    "\n",
    "data = {'Noise Type': noise_types, 'Noise Rate': noise_rate, 'F1 Score': f1_scores}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
