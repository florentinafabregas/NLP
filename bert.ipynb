{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, DataCollatorForTokenClassification, Trainer\n",
    "import evaluate\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from huggingface_hub import notebook_login, Repository, get_full_repo_name\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_og(file_path):\n",
    "    # Initialize lists to store data\n",
    "    sentence_ids = []\n",
    "    tokens = []\n",
    "    # pos_tags = []\n",
    "    # chunk_tags = []\n",
    "    ner_tags = []\n",
    "\n",
    "    # Initialize list to store sentences\n",
    "    sentences = []\n",
    "\n",
    "    # Open the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Initialize sentence ID counter\n",
    "        sentence_id = 0\n",
    "\n",
    "        # Initialize lists to store sentence-level data\n",
    "        sentence_tokens = []\n",
    "        # sentence_pos_tags = []\n",
    "        # sentence_chunk_tags = []\n",
    "        sentence_ner_tags = []\n",
    "\n",
    "        # Iterate through lines\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                if sentence_tokens:  # If the sentence has tokens\n",
    "                    # Append sentence data to lists\n",
    "                    sentence_ids.append(sentence_id)\n",
    "                    tokens.append(sentence_tokens)\n",
    "                    # pos_tags.append(sentence_pos_tags)\n",
    "                    # chunk_tags.append(sentence_chunk_tags)\n",
    "                    ner_tags.append(sentence_ner_tags)\n",
    "                    sentences.append(sentence_tokens)  # Add to sentences list\n",
    "\n",
    "                    # Reset for the next sentence\n",
    "                    sentence_tokens = []\n",
    "                    # sentence_pos_tags = []\n",
    "                    # sentence_chunk_tags = []\n",
    "                    sentence_ner_tags = []\n",
    "                sentence_id += 1  # Increment sentence ID\n",
    "                continue\n",
    "\n",
    "            # # Skip the initial -DOCSTART- line\n",
    "            # if line.startswith('-DOCSTART-'):\n",
    "            #     continue\n",
    "\n",
    "            # Split line by whitespace\n",
    "            parts = line.split()\n",
    "\n",
    "            # Extract data\n",
    "            token = parts[0]\n",
    "            ner_tag = parts[3]\n",
    "\n",
    "            # Append data to sentence-level lists\n",
    "            sentence_tokens.append(token)\n",
    "            # sentence_pos_tags.append(0)  # Append 0 for pos_tags\n",
    "            # sentence_chunk_tags.append(0)  # Append 0 for chunk_tags\n",
    "            sentence_ner_tags.append(ner_tag)\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        'sentence_id': sentence_ids,\n",
    "        'tokens': tokens,\n",
    "        # 'pos_tags': pos_tags,\n",
    "        # 'chunk_tags': chunk_tags,\n",
    "        'ner_tags': ner_tags\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary for now\n",
    "\n",
    "# Define dataset paths\n",
    "traindatapath = \"data/train.txt\"\n",
    "devdatapath = \"data/dev.txt\"\n",
    "testdatapath = \"data/test.txt\"\n",
    "\n",
    "train_df, train_sentences = read_data_og(traindatapath)\n",
    "val_df, val_sentences = read_data_og(devdatapath)\n",
    "test_df, test_sentences = read_data_og(testdatapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique ner tags\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "label2id_int = {'O': 0,\n",
    " 'B-PER': 1,\n",
    " 'I-PER': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-LOC': 5,\n",
    " 'I-LOC': 6,\n",
    " 'B-MISC': 7,\n",
    " 'I-MISC': 8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        \n",
    "        if word_id == None:\n",
    "            current_word = word_id\n",
    "            new_labels.append(-100) # -100 is ignored\n",
    "        \n",
    "        elif word_id != previous_word_idx:\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "        else:\n",
    "            new_labels.append(-100)\n",
    "        previous_word_idx = word_id\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(data):\n",
    "    tokenized_inputs = tokenizer(data['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    all_labels = data['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ner_tags_to_ids(data):\n",
    "    data['ner_tags'] = [[label2id_int[tag] for tag in tags] for tags in data['ner_tags']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a0d0bc457f4ba69143e181bc567eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f926ce489f3643d7aeb2590f2f9ed98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dd10df195e4e229e0d4df17ea477a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ba6adff4a0453aaac9b58f608d3482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df, train_sentences = read_data_og(traindatapath)\n",
    "val_df, val_sentences = read_data_og(devdatapath)\n",
    "test_df, test_sentences = read_data_og(testdatapath)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "train_dataset = train_dataset.map(map_ner_tags_to_ids, batched=True)\n",
    "test_dataset = test_dataset.map(map_ner_tags_to_ids, batched=True)\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn=data_collator)\n",
    "# val_loader = DataLoader(tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator)\n",
    "# test_loader = DataLoader(tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {label: i for i, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name,id2label = id2label, label2id=label2id, num_labels=len(label2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels to a list of lists if it's a set\n",
    "    if isinstance(labels, set):\n",
    "        labels = [labels]\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000002589A1B2500>\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 2e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerator & LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator()\n",
    "\n",
    "# model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "#     model, optimizer, train_loader, val_loader\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_train_epochs = 5\n",
    "# num_update_steps_per_epoch = len(train_loader)\n",
    "# num_training_steps = num_train_epochs * len(train_loader)\n",
    "\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_saved = \"bert-finetuned-ner-Raivis\"\n",
    "# repo_name = get_full_repo_name(model_saved)\n",
    "# repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HF_HOME\"] = \"true\"\n",
    "# output_dir = \"bert-finetuned-ner-Raivis\"\n",
    "# # repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raivi\\AppData\\Local\\Temp\\ipykernel_55344\\3730763129.py:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205a2fff99a14d7e985e9f8391aaeed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1593, 'learning_rate': 1.7865528281750267e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8b11ba419b429d90fa5af66ace8ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09269918501377106, 'eval_precision': 0.89281342892114, 'eval_recall': 0.9040368271954674, 'eval_f1': 0.8983900765373449, 'eval_accuracy': 0.980842583465478, 'eval_runtime': 24.6742, 'eval_samples_per_second': 149.306, 'eval_steps_per_second': 9.362, 'epoch': 1.0}\n",
      "{'loss': 0.0474, 'learning_rate': 1.5731056563500536e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0263, 'learning_rate': 1.3596584845250803e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9784e0bc631944fcbda88d40a7fff0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.109897680580616, 'eval_precision': 0.8945081682307959, 'eval_recall': 0.9112960339943342, 'eval_f1': 0.9028240659533415, 'eval_accuracy': 0.9811211588736982, 'eval_runtime': 27.3821, 'eval_samples_per_second': 134.54, 'eval_steps_per_second': 8.436, 'epoch': 2.0}\n",
      "{'loss': 0.0225, 'learning_rate': 1.146211312700107e-05, 'epoch': 2.13}\n",
      "{'loss': 0.015, 'learning_rate': 9.327641408751335e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1511fff4a376423f9e9f5e4350d19b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11802811920642853, 'eval_precision': 0.9006588072122053, 'eval_recall': 0.9197946175637394, 'eval_f1': 0.9101261387526278, 'eval_accuracy': 0.9821926027514679, 'eval_runtime': 29.2153, 'eval_samples_per_second': 126.098, 'eval_steps_per_second': 7.907, 'epoch': 3.0}\n",
      "{'loss': 0.0122, 'learning_rate': 7.193169690501601e-06, 'epoch': 3.2}\n",
      "{'loss': 0.0079, 'learning_rate': 5.058697972251868e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff8755315b54cd8b243d8c6a73ad91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1261548548936844, 'eval_precision': 0.9049529124520405, 'eval_recall': 0.9187322946175638, 'eval_f1': 0.9117905464768933, 'eval_accuracy': 0.9825354647923542, 'eval_runtime': 44.9737, 'eval_samples_per_second': 81.915, 'eval_steps_per_second': 5.136, 'epoch': 4.0}\n",
      "{'loss': 0.0071, 'learning_rate': 2.924226254002135e-06, 'epoch': 4.27}\n",
      "{'loss': 0.0047, 'learning_rate': 7.897545357524014e-07, 'epoch': 4.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecddee3f69d48f0a1856618945ed601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13258010149002075, 'eval_precision': 0.9027561102444098, 'eval_recall': 0.9220963172804533, 'eval_f1': 0.9123237277743717, 'eval_accuracy': 0.9826426091801311, 'eval_runtime': 36.8013, 'eval_samples_per_second': 100.105, 'eval_steps_per_second': 6.277, 'epoch': 5.0}\n",
      "{'train_runtime': 2152.8953, 'train_samples_per_second': 34.807, 'train_steps_per_second': 2.176, 'train_loss': 0.032454449669910214, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4685, training_loss=0.032454449669910214, metrics={'train_runtime': 2152.8953, 'train_samples_per_second': 34.807, 'train_steps_per_second': 2.176, 'train_loss': 0.032454449669910214, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved = \"bert-finetuned-ner-Raivis\"\n",
    "args = TrainingArguments(\n",
    "    model_saved,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    optimizers = (optimizer, None)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"models/bert-ner-Raivis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811c9a4a22da448c95dd28692339047c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.9291244788564622,\n",
       "  'recall': 0.935251798561151,\n",
       "  'f1': 0.9321780699133552,\n",
       "  'number': 1668},\n",
       " 'MISC': {'precision': 0.7712765957446809,\n",
       "  'recall': 0.8262108262108262,\n",
       "  'f1': 0.797799174690509,\n",
       "  'number': 702},\n",
       " 'ORG': {'precision': 0.8752886836027713,\n",
       "  'recall': 0.9127031908488862,\n",
       "  'f1': 0.8936044798113763,\n",
       "  'number': 1661},\n",
       " 'PER': {'precision': 0.9663760896637609,\n",
       "  'recall': 0.9598021026592455,\n",
       "  'f1': 0.9630778777536456,\n",
       "  'number': 1617},\n",
       " 'overall_precision': 0.9027561102444098,\n",
       " 'overall_recall': 0.9220963172804533,\n",
       " 'overall_f1': 0.9123237277743717,\n",
       " 'overall_accuracy': 0.9826426091801311}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(tokenized_test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [\n",
    "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_output(sentences, predictions):\n",
    "    formatted_data = []\n",
    "    for sentence, prediction in zip(sentences, predictions):\n",
    "        for word, tag in zip(sentence, prediction):\n",
    "            formatted_data.append([word, tag])\n",
    "        # Add an empty line after each sentence\n",
    "        formatted_data.append([\"\", \"\"])\n",
    "    return formatted_data\n",
    "\n",
    "formatted_output = convert_to_output(test_sentences, true_predictions)\n",
    "\n",
    "# Save formatted data to output file in format word tag, sentences are separated by empty line\n",
    "def save_to_output_file(formatted_data, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for data in formatted_data:\n",
    "            file.write(data[0] + '\\t' + data[1] + '\\n')\n",
    "        file.write('\\n')  # Add an empty line after each sentence\n",
    "\n",
    "outputname = \"bert_gold_22052024.txt\"\n",
    "save_to_output_file(formatted_output, outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rob\n",
    "\n",
    "def readBIO(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[1])\n",
    "    return ents\n",
    "\n",
    "def toSpans(tags):\n",
    "    spans = set()\n",
    "    for beg in range(len(tags)):\n",
    "        if tags[beg][0] == 'B':\n",
    "            end = beg\n",
    "            for end in range(beg+1, len(tags)):\n",
    "                if tags[end][0] != 'I':\n",
    "                    break\n",
    "            spans.add(str(beg) + '-' + str(end) + ':' + tags[beg][2:])\n",
    "            #print(end-beg)\n",
    "    return spans\n",
    "\n",
    "def getInstanceScores(predPath, goldPath):\n",
    "    goldEnts = readBIO(goldPath)\n",
    "    predEnts = readBIO(predPath)\n",
    "    entScores = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for goldEnt, predEnt in zip(goldEnts, predEnts):\n",
    "        goldSpans = toSpans(goldEnt)\n",
    "        predSpans = toSpans(predEnt)\n",
    "        overlap = len(goldSpans.intersection(predSpans))\n",
    "        tp += overlap\n",
    "        fp += len(predSpans) - overlap\n",
    "        fn += len(goldSpans) - overlap\n",
    "        \n",
    "    prec = 0.0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    rec = 0.0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0.0 if prec+rec == 0.0 else 2 * (prec * rec) / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9165273990676401\n"
     ]
    }
   ],
   "source": [
    "bert_InstanceScores = getInstanceScores(\"bert_gold_22052024.txt\", \"data/gold_bert.txt\")\n",
    "print(bert_InstanceScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rates = {\n",
    "    'capitalization_swap': [0.1, 0.15, 0.2, 0.25, 0.3], \n",
    "    'character_swap': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_removal': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_replacement': [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_data_grouped(file_path):\n",
    "#     # Initialize lists to store data\n",
    "#     sentence_ids = []\n",
    "#     tokens = []\n",
    "#     pos_tags = []\n",
    "#     chunk_tags = []\n",
    "#     ner_tags = []\n",
    "\n",
    "#     # Initialize list to store sentences\n",
    "#     sentences = []\n",
    "\n",
    "#     # Open the file\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         # Initialize sentence ID counter\n",
    "#         sentence_id = 0\n",
    "\n",
    "#         # Initialize lists to store sentence-level data\n",
    "#         sentence_tokens = []\n",
    "#         sentence_pos_tags = []\n",
    "#         sentence_chunk_tags = []\n",
    "#         sentence_ner_tags = []\n",
    "\n",
    "#         # Iterate through lines\n",
    "#         for line in file:\n",
    "#             line = line.strip()\n",
    "\n",
    "#             # Skip empty lines\n",
    "#             if not line:\n",
    "#                 if sentence_tokens:  # If the sentence has tokens\n",
    "#                     # Append sentence data to lists\n",
    "#                     sentence_ids.append(sentence_id)\n",
    "#                     tokens.append(sentence_tokens)\n",
    "#                     pos_tags.append(sentence_pos_tags)\n",
    "#                     chunk_tags.append(sentence_chunk_tags)\n",
    "#                     ner_tags.append(sentence_ner_tags)\n",
    "#                     sentences.append(sentence_tokens)  # Add to sentences list\n",
    "\n",
    "#                     # Reset for the next sentence\n",
    "#                     sentence_tokens = []\n",
    "#                     sentence_pos_tags = []\n",
    "#                     sentence_chunk_tags = []\n",
    "#                     sentence_ner_tags = []\n",
    "#                 sentence_id += 1  # Increment sentence ID\n",
    "#                 continue\n",
    "\n",
    "#             # # Skip the initial -DOCSTART- line\n",
    "#             # if line.startswith('-DOCSTART-'):\n",
    "#             #     continue\n",
    "\n",
    "#             # Split line by whitespace\n",
    "#             parts = line.split()\n",
    "\n",
    "#             # Extract data\n",
    "#             token = parts[0]\n",
    "#             ner_tag = parts[1]\n",
    "\n",
    "#             # Append data to sentence-level lists\n",
    "#             sentence_tokens.append(token)\n",
    "#             sentence_pos_tags.append(0)  # Append 0 for pos_tags\n",
    "#             sentence_chunk_tags.append(0)  # Append 0 for chunk_tags\n",
    "#             sentence_ner_tags.append(ner_tag)\n",
    "\n",
    "#     # Create DataFrame\n",
    "#     data = {\n",
    "#         'sentence_id': sentence_ids,\n",
    "#         'tokens': tokens,\n",
    "#         'pos_tags': pos_tags,\n",
    "#         'chunk_tags': chunk_tags,\n",
    "#         'ner_tags': ner_tags\n",
    "#     }\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "#     return df, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for noise_type in noise_rates.keys():\n",
    "#     for rate in noise_rates[noise_type]:\n",
    "\n",
    "#         path = f'data/altered/{noise_type}_rate_{rate}.txt'\n",
    "#         outpath = f'predictions/altered/bert/{noise_type}_rate_{rate}.txt'\n",
    "\n",
    "#         test_df , sentences = read_data_grouped(path)\n",
    "#         test_dataset = Dataset.from_pandas(test_df)\n",
    "#         test_dataset = test_dataset.map(map_ner_tags_to_ids, batched=True)\n",
    "#         tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "#         predictions, labels, metrics = trainer.predict(tokenized_test_dataset)\n",
    "#         predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "#         true_predictions = [\n",
    "#             [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#             for prediction, label in zip(predictions, labels)\n",
    "#         ]\n",
    "#         true_labels = [\n",
    "#             [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#             for prediction, label in zip(predictions, labels)\n",
    "#         ]\n",
    "\n",
    "#         output = convert_to_output(sentences, true_predictions)\n",
    "#         save_to_output_file(output,outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types = []\n",
    "# rates = []\n",
    "# f1_score = []\n",
    "\n",
    "# for noise_type in noise_rates.keys():\n",
    "#     for rate in noise_rates[noise_type]:\n",
    "\n",
    "#         score = getInstanceScores(f\"predictions/altered/bert/{noise_type}_rate_{rate}.txt\", \"data/gold.txt\")\n",
    "        \n",
    "#         # Append data to lists\n",
    "#         types.append(noise_type)\n",
    "#         rates.append(rate)\n",
    "#         f1_score.append(score)\n",
    "\n",
    "# results = {'Type': types, 'Rate': rates, 'F1 Score': f1_score}\n",
    "# df = pd.DataFrame(results)\n",
    "# print(df)\n",
    "\n",
    "# df.to_csv('out/df_altered_bert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
