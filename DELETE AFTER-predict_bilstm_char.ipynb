{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionaries\n",
    "with open('models/bilstm_char/id2word.pkl', 'rb') as f:\n",
    "    id2word = pickle.load(f)\n",
    "with open('models/bilstm_char/id2tag.pkl', 'rb') as f:\n",
    "    id2tag = pickle.load(f)\n",
    "with open('models/bilstm_char/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open('models/bilstm_char/nertags.pkl', 'rb') as f:\n",
    "    nertags = pickle.load(f)\n",
    "with open('models/bilstm_char/char_vocab.pkl', 'rb') as f:\n",
    "    char_vocab = pickle.load(f)\n",
    "\n",
    "# Load pretrained embeddings\n",
    "word_embeds = torch.load('models/bilstm_char/pretrained_embeddings.pt')\n",
    "char_onehot = torch.load('models/bilstm_char/pretrained_char_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM for CHARACTER level\n",
    "\n",
    "class forLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, pretr_char_embed):\n",
    "        super(forLSTM, self).__init__()\n",
    "        self.charembed = nn.Embedding.from_pretrained(pretr_char_embed, freeze = False) #size of pretrained = (totalchars,embedding size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, bidirectional = True, batch_first = True)\n",
    "\n",
    "    def forward(self, xchar, xlength_char):\n",
    "        shape = xchar.shape\n",
    "        xchar = xchar.view(-1, shape[2])\n",
    "        xlength_char = xlength_char.view(-1)\n",
    "        \n",
    "        input = pack_padded_sequence(xchar, xlength_char.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        input, _ = pad_packed_sequence(input, batch_first=True)\n",
    "        embed = self.charembed(input)\n",
    "        _, (h,_) = self.lstm(embed) #h is of size (2, 128*maxno. of words in a sentence in the batch, 25)\n",
    "        h = h.view(h.shape[1], 50)\n",
    "        h = h.view(shape[0], shape[1], 50)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BILSTM model\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, total_words, num_class, pretrained = False, pretrained_embed = None, char_embed_size = 0, pretr_char_embed = None):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wordembed = nn.Embedding.from_pretrained(pretrained_embed, freeze = False)\n",
    "        self.for_charembed = forLSTM(embedding_size = char_embed_size, hidden_size = 25, pretr_char_embed = pretr_char_embed)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.bilstm = nn.LSTM(embedding_size + 50,hidden_size, bidirectional = True, batch_first = True)\n",
    "        self.linear = nn.Linear(2*hidden_size, num_class) # 2 because forward and backward concatenate\n",
    "\n",
    "    def forward(self, x, xchar, xlengths, xlength_char):\n",
    "        x = pack_padded_sequence(x, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x, _ = pad_packed_sequence(x, batch_first=True)\n",
    "\n",
    "        xlength_char = pack_padded_sequence(xlength_char, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        xlength_char, _ = pad_packed_sequence(xlength_char, batch_first=True, padding_value = len(\"<pad>\")) \n",
    "        # above this line padding value is taken as len of pad word becasue that is what we pad sentences \n",
    "        # with hance at a character level it should be the length\n",
    "\n",
    "        xchar = pack_padded_sequence(xchar, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        xchar, _ = pad_packed_sequence(xchar, batch_first=True)\n",
    "\n",
    "        word_embedding = self.wordembed(x) # x is of size(batchsize, seq_len), wordembed is of size (batchsize, seq_len, embedding_size = 100)\n",
    "        forwardchar= self.for_charembed(xchar, xlength_char) #forwardchar and backwardchar would be of size (batchsize, seqlen. embedding_size = 25each) \n",
    "        word_embedding = torch.cat((word_embedding, forwardchar), dim = 2)\n",
    "\n",
    "        word_embedding = self.dropout(word_embedding) #dropout\n",
    "        out, (h,c) = self.bilstm(word_embedding) #'out' has dimension(batchsize, seq_len, 2*hidden_size)\n",
    "\n",
    "        out = self.linear(out) #now 'out' has dimension(batchsize, seq_len, num_class)\n",
    "        out = out.view(-1, out.shape[2]) # shape (128*seqlen, 18)\n",
    "        out = F.log_softmax(out, dim=1) # take the softmax across the dimension num_class, 'out' has dimension(batchsize, seq_len, num_class)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_char_data(words, charvocab):\n",
    "    train_char_sent = []\n",
    "    train_char_label = []\n",
    "    for word in words:\n",
    "        chars = []\n",
    "        char_labels = []\n",
    "\n",
    "        word_sep = list(word)\n",
    "        for c in word_sep[:-1]:\n",
    "            if (c in charvocab.keys()):\n",
    "                chars.append(charvocab[c])\n",
    "            else:\n",
    "                chars.append(charvocab['<oovchar>'])\n",
    "        for c in word_sep[1:]:\n",
    "            if (c in charvocab.keys()):\n",
    "                char_labels.append(charvocab[c])\n",
    "            else:\n",
    "                char_labels.append(charvocab['<oovchar>'])\n",
    "        \n",
    "        train_char_sent.append(chars)\n",
    "        train_char_label.append(char_labels)\n",
    "\n",
    "    # padding the char_sents at the end\n",
    "    seq_maxlen = max(len(x) for x in train_char_sent)\n",
    "    x_lengths_char = [len(x) for x in train_char_sent]\n",
    "    Xtrain_char = []\n",
    "    Ytrain_char = []\n",
    "    for char_sent, char_label in zip(train_char_sent, train_char_label):\n",
    "        length_toappend = seq_maxlen - len(char_sent)\n",
    "        Xtrain_char.append(char_sent+[0]*length_toappend)\n",
    "        Ytrain_char.append(char_label+[0]*length_toappend) # 0 is padchar\n",
    "\n",
    "\n",
    "    Xtrain_char = torch.Tensor(Xtrain_char)\n",
    "    Ytrain_char = torch.Tensor(Ytrain_char)\n",
    "    x_lengths_char = torch.Tensor(x_lengths_char)\n",
    "    # print(Xtrain.shape, Ytrain.shape, x_lengths.shape)\n",
    "    \n",
    "    return Xtrain_char, Ytrain_char, x_lengths_char\n",
    "\n",
    "def pad_chars(topadlist, maxlen):\n",
    "    topadlist = topadlist + [0]*(maxlen-len(topadlist))\n",
    "\n",
    "    return topadlist\n",
    "\n",
    "def make_id2word_charvocab(vocab, charvocab):\n",
    "    max_charlen = max(len(word) for word in vocab.keys())\n",
    "    word_charlevel_vocab = {}\n",
    "    wordid2wordlen = {}\n",
    "    for word in vocab.keys():\n",
    "        word_charlevel_vocab[vocab[word]] = [charvocab[w] if w in charvocab.keys() else charvocab['<oovchar>'] for w in word]\n",
    "        word_charlevel_vocab[vocab[word]] = pad_chars(word_charlevel_vocab[vocab[word]], max_charlen)\n",
    "\n",
    "        wordid2wordlen[vocab[word]] = len(word)\n",
    "        # word_charlevel_vocab[vocab[word]] = word_charlevel_vocab[vocab[word]].extend([charvocab['<padchar>']]*(max_charlen-len(word_charlevel_vocab[vocab[word]])))\n",
    "    return word_charlevel_vocab, wordid2wordlen\n",
    "\n",
    "\n",
    "def load_char_level(X, wordid2word_charlevel_vocab, wordid2wordlen):\n",
    "    #X is of shape (no.of.sentences, 104)\n",
    "    Xcharlevel = [] # will finally be fo shape (total.sentences, max_sent.len, )\n",
    "    Xcharlevel_lengths = []\n",
    "    for i in range(X.shape[0]):\n",
    "        sentence = []\n",
    "        wordlengths = []\n",
    "        for j in range(X.shape[1]):\n",
    "            sentence.append(torch.tensor([wordid2word_charlevel_vocab[int(X[i, j].item())]]))\n",
    "            wordlengths.append(wordid2wordlen[int(X[i, j].item())])\n",
    "            # sentences = pad_sequence(sentences)\n",
    "        # print(i)\n",
    "        Xcharlevel_lengths.append(wordlengths)\n",
    "        Xcharlevel.append(torch.stack(sentence))\n",
    "    \n",
    "    return torch.squeeze(torch.stack(Xcharlevel)), torch.tensor(Xcharlevel_lengths)\n",
    "\n",
    "def get_charvocab(vocab):\n",
    "    # using vocab make charvocab\n",
    "    words = list(vocab.keys())\n",
    "    characters = [char for word in words for char in word]\n",
    "    characters = list(set(characters))\n",
    "    char_vocab = {}\n",
    "    char_vocab[\"<padchar>\"] = 0\n",
    "    char_vocab[\"<oovchar>\"] = 1\n",
    "    for i, char in enumerate(characters):\n",
    "        char_vocab[char] = i+2\n",
    "\n",
    "    return char_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_data(datapath, buildvocab_tags= True, vocab = None, nertags = None):\n",
    "    if(buildvocab_tags == True):\n",
    "        all_words = []\n",
    "        all_tags = []\n",
    "        with open(datapath) as f:\n",
    "            lines = f.readlines()\n",
    "            sent_num = 0\n",
    "            for line in lines: #1: so that the first blank line isn't taken into account\n",
    "                if(line == \"\\n\"):\n",
    "                    sent_num+=1\n",
    "                else:\n",
    "                    line_sep = line.split('\\t')\n",
    "                    all_words.append(line_sep[0])\n",
    "                    all_tags.append(line_sep[1][:-1])\n",
    "                    \n",
    "        words = list(set(all_words))\n",
    "        tags = list(set(all_tags))\n",
    "\n",
    "        vocab = {}\n",
    "        vocab['<pad>'] = 0 # for padding input sequences\n",
    "        vocab['<oov>'] = 1\n",
    "        for i, word in enumerate(words):\n",
    "            vocab[word] = i+2\n",
    "            \n",
    "        nertags = {}\n",
    "        nertags['padtag'] = 0\n",
    "        for i,nertag in enumerate(tags):\n",
    "            nertags[nertag] = i+1\n",
    "\n",
    "    train_sent = []\n",
    "    train_tags = []\n",
    "    with open(datapath) as f:\n",
    "        lines = f.readlines()\n",
    "        sent_num = 0\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        for line in lines: #1: so that the first blank line isn't taken into account\n",
    "            if(line == \"\\n\"):\n",
    "                sent_num+=1\n",
    "                train_sent.append(sentence)\n",
    "                train_tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                line_sep = line.split(\"\\t\")\n",
    "                if(line_sep[0] in vocab.keys()):\n",
    "                    sentence.append(vocab[line_sep[0]])\n",
    "                else:\n",
    "                    sentence.append(vocab['<oov>'])  \n",
    "                tag.append(nertags[line_sep[-1][:-1]])\n",
    "\n",
    "    # padding the sentences at the end\n",
    "    seq_maxlen = max(len(x) for x in train_sent)\n",
    "    x_lengths = [len(x) for x in train_sent]\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    for sent, tags in zip(train_sent, train_tags):\n",
    "        length_toappend = seq_maxlen - len(sent)\n",
    "        Xtrain.append(sent+[0]*length_toappend)\n",
    "        Ytrain.append(tags+[0]*length_toappend)\n",
    "\n",
    "\n",
    "    Xtrain = torch.Tensor(Xtrain)\n",
    "    Ytrain = torch.Tensor(Ytrain)\n",
    "    x_lengths = torch.Tensor(x_lengths)\n",
    "    \n",
    "    return Xtrain, Ytrain, x_lengths, vocab, nertags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (wordembed): Embedding(23626, 100)\n",
       "  (for_charembed): forLSTM(\n",
       "    (charembed): Embedding(88, 88)\n",
       "    (lstm): LSTM(88, 25, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bilstm): LSTM(150, 100, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM(embedding_size = 100, hidden_size = 100, total_words = len(vocab), num_class = len(nertags),  pretrained=True, pretrained_embed = word_embeds, char_embed_size = len(char_vocab),pretr_char_embed = char_onehot) \n",
    "model.load_state_dict(torch.load('models/bilstm_char/trained_bilstm_model_state_dict.pth'))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(testdatapath):\n",
    "    Xtest, Ytest, x_testlengths, _, _ = test_load_data(testdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)\n",
    "\n",
    "    Xtest_temp, Ytest_temp, x_testlengths_temp, testvocab, testnertags = test_load_data(testdatapath, buildvocab_tags=True)\n",
    "    wordid2word_charlevel_vocab_test, wordid2wordlen_test = make_id2word_charvocab(testvocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
    "    #make char level train data for the char embeddings \n",
    "    Xtest_char, xtestlength_char = load_char_level(Xtest_temp, wordid2word_charlevel_vocab_test, wordid2wordlen_test)\n",
    "    #finally make the dataloader for train\n",
    "    testdataset = TensorDataset(Xtest, Xtest_char, Ytest, x_testlengths, xtestlength_char)\n",
    "    loader_test = DataLoader(testdataset, batch_size= 1, shuffle=False)\n",
    "    return loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "    y_predicted = []\n",
    "    with torch.no_grad():\n",
    "        for step, (X, Xchar, Y, xlen, xlen_char) in enumerate(loader):\n",
    "            ypred = model(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device))#.permute(0, 2, 1)\n",
    "            ypred = torch.argmax(ypred.to('cpu'), dim = 1)\n",
    "            ypred = ypred.view(Y.shape[0], -1)\n",
    "            y_predicted.append(ypred)\n",
    "\n",
    "    y_predicted_list = []\n",
    "    for i in range(len(y_predicted)):\n",
    "        for j in range(y_predicted[i].shape[0]):\n",
    "            sent_pred = []\n",
    "            for x in range(y_predicted[i].shape[1]):\n",
    "                sent_pred.append(id2tag[int(y_predicted[i][j, x])])\n",
    "            y_predicted_list.append(sent_pred)\n",
    "    return y_predicted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writefile(testfilepath, outputfilepath, predictions):\n",
    "    final_output = [] #list of lists which will finally be written to file\n",
    "    with open(testfilepath) as f:\n",
    "        lines = f.readlines()\n",
    "        sentnum = 0 #to take care of the first blank line\n",
    "        wordnum = 0\n",
    "        for line in lines:\n",
    "            if(line == '\\n'):\n",
    "                sentnum+=1\n",
    "                wordnum = 0\n",
    "                final_output.append(line)\n",
    "\n",
    "            else:\n",
    "                line_sep = line.split(sep = \"\\t\")\n",
    "                word = line_sep[0]\n",
    "                prediction = predictions[sentnum][wordnum]\n",
    "                final_output.append(f'{word}\\t{prediction}\\n')\n",
    "                wordnum+=1\n",
    "    #write the outputfilepath\n",
    "    with open(outputfilepath, 'w+') as f:\n",
    "        f.writelines(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPAN-F1 score\n",
    "def readBIO(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[1])\n",
    "    return ents\n",
    "\n",
    "def toSpans(tags):\n",
    "    spans = set()\n",
    "    for beg in range(len(tags)):\n",
    "        if tags[beg][0] == 'B':\n",
    "            end = beg\n",
    "            for end in range(beg+1, len(tags)):\n",
    "                if tags[end][0] != 'I':\n",
    "                    break\n",
    "            spans.add(str(beg) + '-' + str(end) + ':' + tags[beg][2:])\n",
    "            #print(end-beg)\n",
    "    return spans\n",
    "\n",
    "def getInstanceScores(predPath, goldPath):\n",
    "    goldEnts = readBIO(goldPath)\n",
    "    predEnts = readBIO(predPath)\n",
    "    entScores = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for goldEnt, predEnt in zip(goldEnts, predEnts):\n",
    "        goldSpans = toSpans(goldEnt)\n",
    "        predSpans = toSpans(predEnt)\n",
    "        overlap = len(goldSpans.intersection(predSpans))\n",
    "        tp += overlap\n",
    "        fp += len(predSpans) - overlap\n",
    "        fn += len(goldSpans) - overlap\n",
    "        \n",
    "    prec = 0.0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    rec = 0.0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0.0 if prec+rec == 0.0 else 2 * (prec * rec) / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rates = {\n",
    "    'capitalization_swap': [0.1, 0.15, 0.2, 0.25, 0.3], \n",
    "    'character_swap': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_removal': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_replacement': [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        path = f'data/altered_2/{noise_type}_rate_{rate}.txt'\n",
    "        my_loader = loader(path)\n",
    "        predictions = test_model(model, my_loader)\n",
    "        #print(predictions)\n",
    "        writefile(path, f'predictions/altered_2/char/{noise_type}_rate_{rate}.txt',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Type  Rate  F1 Score\n",
      "0     capitalization_swap  0.10  0.681053\n",
      "1     capitalization_swap  0.15  0.654251\n",
      "2     capitalization_swap  0.20  0.624754\n",
      "3     capitalization_swap  0.25  0.591449\n",
      "4     capitalization_swap  0.30  0.584604\n",
      "5          character_swap  0.10  0.702227\n",
      "6          character_swap  0.15  0.561937\n",
      "7          character_swap  0.20  0.436957\n",
      "8          character_swap  0.25  0.387277\n",
      "9          character_swap  0.30  0.368905\n",
      "10      character_removal  0.10  0.698443\n",
      "11      character_removal  0.15  0.538231\n",
      "12      character_removal  0.20  0.395190\n",
      "13      character_removal  0.25  0.349141\n",
      "14      character_removal  0.30  0.346803\n",
      "15  character_replacement  0.10  0.701609\n",
      "16  character_replacement  0.15  0.538052\n",
      "17  character_replacement  0.20  0.393503\n",
      "18  character_replacement  0.25  0.339022\n",
      "19  character_replacement  0.30  0.340380\n"
     ]
    }
   ],
   "source": [
    "types = []\n",
    "rates = []\n",
    "f1_score = []\n",
    "\n",
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        score = getInstanceScores(f'predictions/altered_2/char/{noise_type}_rate_{rate}.txt', 'data/gold.txt')\n",
    "        \n",
    "        # Append data to lists\n",
    "        types.append(noise_type)\n",
    "        rates.append(rate)\n",
    "        f1_score.append(score)\n",
    "\n",
    "results = {'Type': types, 'Rate': rates, 'F1 Score': f1_score}\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n",
    "df.to_csv('out/df_altered_2_char.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        path = f'data/altered_3/{noise_type}_rate_{rate}.txt'\n",
    "        my_loader = loader(path)\n",
    "        predictions = test_model(model, my_loader)\n",
    "        #print(predictions)\n",
    "        writefile(path, f'predictions/altered_3/char/{noise_type}_rate_{rate}.txt',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Type  Rate  F1 Score\n",
      "0     capitalization_swap  0.10  0.678354\n",
      "1     capitalization_swap  0.15  0.657219\n",
      "2     capitalization_swap  0.20  0.625711\n",
      "3     capitalization_swap  0.25  0.594132\n",
      "4     capitalization_swap  0.30  0.570690\n",
      "5          character_swap  0.10  0.703132\n",
      "6          character_swap  0.15  0.564339\n",
      "7          character_swap  0.20  0.438082\n",
      "8          character_swap  0.25  0.376210\n",
      "9          character_swap  0.30  0.373736\n",
      "10      character_removal  0.10  0.698628\n",
      "11      character_removal  0.15  0.539425\n",
      "12      character_removal  0.20  0.397400\n",
      "13      character_removal  0.25  0.348741\n",
      "14      character_removal  0.30  0.347168\n",
      "15  character_replacement  0.10  0.700847\n",
      "16  character_replacement  0.15  0.537385\n",
      "17  character_replacement  0.20  0.390278\n",
      "18  character_replacement  0.25  0.338835\n",
      "19  character_replacement  0.30  0.338558\n"
     ]
    }
   ],
   "source": [
    "types = []\n",
    "rates = []\n",
    "f1_score = []\n",
    "\n",
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        score = getInstanceScores(f'predictions/altered_3/char/{noise_type}_rate_{rate}.txt', 'data/gold.txt')\n",
    "        \n",
    "        # Append data to lists\n",
    "        types.append(noise_type)\n",
    "        rates.append(rate)\n",
    "        f1_score.append(score)\n",
    "\n",
    "results = {'Type': types, 'Rate': rates, 'F1 Score': f1_score}\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n",
    "df.to_csv('out/df_altered_3_char.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-F1 score:  0.6810616284300495\n"
     ]
    }
   ],
   "source": [
    "noise_type = 'capitalization_swap'\n",
    "rate = 0.1\n",
    "\n",
    "testdatapath = f'data/altered/{noise_type}_rate_{rate}.txt'\n",
    "prediction_path = f'predictions/bilstm_char/{noise_type}_rate_{rate}.txt'\n",
    "\n",
    "my_loader = loader(testdatapath)\n",
    "predictions = test_model(model, my_loader)\n",
    "writefile(testdatapath, prediction_path, predictions)\n",
    "\n",
    "span_f1_score = getInstanceScores(prediction_path,'data/gold.txt')\n",
    "print('Span-F1 score: ', span_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
