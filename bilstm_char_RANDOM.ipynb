{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/HarmanDotpy/Named-Entity-Recognition-in-Pytorch/blob/main/scripts/train_bilstm_char_random_glove.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(10)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import io\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle as pickle\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import seqeval\n",
    "from seqeval.metrics import accuracy_score as seq_accuracy_score\n",
    "from seqeval.metrics import classification_report as seq_classification_report\n",
    "from seqeval.metrics import f1_score as seq_f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM for CHARACTER level\n",
    "\n",
    "class forLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, pretr_char_embed):\n",
    "        super(forLSTM, self).__init__()\n",
    "        self.charembed = nn.Embedding.from_pretrained(pretr_char_embed, freeze = False) #size of pretrained = (totalchars,embedding size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, bidirectional = True, batch_first = True)\n",
    "\n",
    "    def forward(self, xchar, xlength_char):\n",
    "        #xchar is of shape(batchsize, seqlen_maxinbatch, maxwordlen-ie max char = 6)\n",
    "\n",
    "        shape = xchar.shape\n",
    "        xchar = xchar.view(-1, shape[2])\n",
    "        xlength_char = xlength_char.view(-1)\n",
    "        input = pack_padded_sequence(xchar, xlength_char.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        input, _ = pad_packed_sequence(input, batch_first=True)\n",
    "        embed = self.charembed(input)\n",
    "        _, (h,_) = self.lstm(embed) #h is of size (2, 128*maxno. of words in a sentence in the batch, 25)\n",
    "        h = h.view(h.shape[1], 50)\n",
    "        h = h.view(shape[0], shape[1], 50)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BILSTM model\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, total_words, num_class, pretrained = False, pretrained_embed = None, char_embed_size = 0, pretr_char_embed = None):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wordembed = nn.Embedding.from_pretrained(pretrained_embed, freeze = False)\n",
    "        self.for_charembed = forLSTM(embedding_size = char_embed_size, hidden_size = 25, pretr_char_embed = pretr_char_embed)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.bilstm = nn.LSTM(embedding_size + 50,hidden_size, bidirectional = True, batch_first = True)\n",
    "        self.linear = nn.Linear(2*hidden_size, num_class) # 2 because forward and backward concatenate\n",
    "\n",
    "    def forward(self, x, xchar, xlengths, xlength_char):\n",
    "        x = pack_padded_sequence(x, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x, _ = pad_packed_sequence(x, batch_first=True)\n",
    "\n",
    "        xlength_char = pack_padded_sequence(xlength_char, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        xlength_char, _ = pad_packed_sequence(xlength_char, batch_first=True, padding_value = len(\"<pad>\")) \n",
    "        # above this line padding value is taken as len of pad word becasue that is what we pad sentences \n",
    "        # with hance at a character level it should be the length\n",
    "\n",
    "        xchar = pack_padded_sequence(xchar, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        xchar, _ = pad_packed_sequence(xchar, batch_first=True)\n",
    "\n",
    "        word_embedding = self.wordembed(x) # x is of size(batchsize, seq_len), wordembed is of size (batchsize, seq_len, embedding_size = 100)\n",
    "        forwardchar= self.for_charembed(xchar, xlength_char) #forwardchar and backwardchar would be of size (batchsize, seqlen. embedding_size = 25each) \n",
    "        word_embedding = torch.cat((word_embedding, forwardchar), dim = 2)\n",
    "\n",
    "        word_embedding = self.dropout(word_embedding) #dropout\n",
    "        out, (h,c) = self.bilstm(word_embedding) #'out' has dimension(batchsize, seq_len, 2*hidden_size)\n",
    "\n",
    "        out = self.linear(out) #now 'out' has dimension(batchsize, seq_len, num_class)\n",
    "        out = out.view(-1, out.shape[2]) # shape (128*seqlen, 18)\n",
    "        out = F.log_softmax(out, dim=1) # take the softmax across the dimension num_class, 'out' has dimension(batchsize, seq_len, num_class)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading text file in python and making list of sentences (list of lists) and list of tags(list of lists)\n",
    "def load_data(datapath, buildvocab_tags= True, vocab = None, nertags = None):\n",
    "    if(buildvocab_tags == True):\n",
    "        all_words = []\n",
    "        all_tags = []\n",
    "        with open(datapath) as f:\n",
    "            lines = f.readlines()\n",
    "            sent_num = 0\n",
    "            for line in lines[2:]: \n",
    "                if(line == \"\\n\"):\n",
    "                    sent_num+=1\n",
    "                else:\n",
    "                    line_sep = line.split(sep = \" \")\n",
    "                    all_words.append(line_sep[0])\n",
    "                    all_tags.append(line_sep[3][:-1])\n",
    "                    \n",
    "        words = list(set(all_words))\n",
    "        tags = list(set(all_tags))\n",
    "\n",
    "        vocab = {}\n",
    "        vocab['<pad>'] = 0 # for padding input sequences\n",
    "        vocab['<oov>'] = 1\n",
    "        for i, word in enumerate(words):\n",
    "            vocab[word] = i+2\n",
    "            \n",
    "        nertags = {}\n",
    "        nertags['padtag'] = 0\n",
    "        for i,nertag in enumerate(tags):\n",
    "            nertags[nertag] = i+1\n",
    "\n",
    "    train_sent = []\n",
    "    train_tags = []\n",
    "    with open(datapath) as f:\n",
    "        lines = f.readlines()\n",
    "        sent_num = 0\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        for line in lines[2:]: #1: so that the first blank line isn't taken into account\n",
    "            if(line == \"\\n\"):\n",
    "                sent_num+=1\n",
    "                train_sent.append(sentence)\n",
    "                train_tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                line_sep = line.split(sep = \" \")\n",
    "                if(line_sep[0] in vocab.keys()):\n",
    "                    sentence.append(vocab[line_sep[0]])\n",
    "                else:\n",
    "                    sentence.append(vocab['<oov>'])\n",
    "                    \n",
    "                tag.append(nertags[line_sep[3][:-1]])\n",
    "\n",
    "    # padding the sentences at the end\n",
    "    seq_maxlen = max(len(x) for x in train_sent)\n",
    "    x_lengths = [len(x) for x in train_sent]\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    for sent, tags in zip(train_sent, train_tags):\n",
    "        length_toappend = seq_maxlen - len(sent)\n",
    "        Xtrain.append(sent+[0]*length_toappend)\n",
    "        Ytrain.append(tags+[0]*length_toappend)\n",
    "\n",
    "\n",
    "    Xtrain = torch.Tensor(Xtrain)\n",
    "    Ytrain = torch.Tensor(Ytrain)\n",
    "    x_lengths = torch.Tensor(x_lengths)\n",
    "    print(Xtrain.shape, Ytrain.shape, x_lengths.shape)\n",
    "    \n",
    "    return Xtrain, Ytrain, x_lengths, vocab, nertags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_data(datapath, buildvocab_tags= True, vocab = None, nertags = None):\n",
    "    if(buildvocab_tags == True):\n",
    "        all_words = []\n",
    "        all_tags = []\n",
    "        with open(datapath) as f:\n",
    "            lines = f.readlines()\n",
    "            sent_num = 0\n",
    "            for line in lines: #1: so that the first blank line isn't taken into account\n",
    "                if(line == \"\\n\"):\n",
    "                    sent_num+=1\n",
    "                else:\n",
    "                    line_sep = line.split(sep = \" \")\n",
    "                    all_words.append(line_sep[0])\n",
    "                    all_tags.append(line_sep[1])\n",
    "                    \n",
    "        words = list(set(all_words))\n",
    "        tags = list(set(all_tags))\n",
    "\n",
    "        vocab = {}\n",
    "        vocab['<pad>'] = 0 # for padding input sequences\n",
    "        vocab['<oov>'] = 1\n",
    "        for i, word in enumerate(words):\n",
    "            vocab[word] = i+2\n",
    "            \n",
    "        nertags = {}\n",
    "        nertags['padtag'] = 0\n",
    "        for i,nertag in enumerate(tags):\n",
    "            nertags[nertag] = i+1\n",
    "\n",
    "    train_sent = []\n",
    "    train_tags = []\n",
    "    with open(datapath) as f:\n",
    "        lines = f.readlines()\n",
    "        sent_num = 0\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        for line in lines: #1: so that the first blank line isn't taken into account\n",
    "            if(line == \"\\n\"):\n",
    "                sent_num+=1\n",
    "                train_sent.append(sentence)\n",
    "                train_tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                line_sep = line.split(sep = \" \")\n",
    "                if(line_sep[0] in vocab.keys()):\n",
    "                    sentence.append(vocab[line_sep[0]])\n",
    "                else:\n",
    "                    sentence.append(vocab['<oov>'])\n",
    "                    \n",
    "                tag.append(nertags[line_sep[1]])\n",
    "\n",
    "    # padding the sentences at the end\n",
    "    seq_maxlen = max(len(x) for x in train_sent)\n",
    "    x_lengths = [len(x) for x in train_sent]\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    for sent, tags in zip(train_sent, train_tags):\n",
    "        length_toappend = seq_maxlen - len(sent)\n",
    "        Xtrain.append(sent+[0]*length_toappend)\n",
    "        Ytrain.append(tags+[0]*length_toappend)\n",
    "\n",
    "\n",
    "    Xtrain = torch.Tensor(Xtrain)\n",
    "    Ytrain = torch.Tensor(Ytrain)\n",
    "    x_lengths = torch.Tensor(x_lengths)\n",
    "    #print(Xtrain.shape, Ytrain.shape, x_lengths.shape)\n",
    "    \n",
    "    return Xtrain, Ytrain, x_lengths, vocab, nertags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_char_data(words, charvocab):\n",
    "    train_char_sent = []\n",
    "    train_char_label = []\n",
    "    for word in words:\n",
    "        chars = []\n",
    "        char_labels = []\n",
    "\n",
    "        word_sep = list(word)\n",
    "        for c in word_sep[:-1]:\n",
    "            if (c in charvocab.keys()):\n",
    "                chars.append(charvocab[c])\n",
    "            else:\n",
    "                chars.append(charvocab['<oovchar>'])\n",
    "        for c in word_sep[1:]:\n",
    "            if (c in charvocab.keys()):\n",
    "                char_labels.append(charvocab[c])\n",
    "            else:\n",
    "                char_labels.append(charvocab['<oovchar>'])\n",
    "        \n",
    "        train_char_sent.append(chars)\n",
    "        train_char_label.append(char_labels)\n",
    "\n",
    "    # padding the char_sents at the end\n",
    "    seq_maxlen = max(len(x) for x in train_char_sent)\n",
    "    x_lengths_char = [len(x) for x in train_char_sent]\n",
    "    Xtrain_char = []\n",
    "    Ytrain_char = []\n",
    "    for char_sent, char_label in zip(train_char_sent, train_char_label):\n",
    "        length_toappend = seq_maxlen - len(char_sent)\n",
    "        Xtrain_char.append(char_sent+[0]*length_toappend)\n",
    "        Ytrain_char.append(char_label+[0]*length_toappend) # 0 is padchar\n",
    "\n",
    "\n",
    "    Xtrain_char = torch.Tensor(Xtrain_char)\n",
    "    Ytrain_char = torch.Tensor(Ytrain_char)\n",
    "    x_lengths_char = torch.Tensor(x_lengths_char)\n",
    "    # print(Xtrain.shape, Ytrain.shape, x_lengths.shape)\n",
    "    \n",
    "    return Xtrain_char, Ytrain_char, x_lengths_char\n",
    "\n",
    "def pad_chars(topadlist, maxlen):\n",
    "    topadlist = topadlist + [0]*(maxlen-len(topadlist))\n",
    "\n",
    "    return topadlist\n",
    "\n",
    "def make_id2word_charvocab(vocab, charvocab):\n",
    "    max_charlen = max(len(word) for word in vocab.keys())\n",
    "    word_charlevel_vocab = {}\n",
    "    wordid2wordlen = {}\n",
    "    for word in vocab.keys():\n",
    "        word_charlevel_vocab[vocab[word]] = [charvocab[w] if w in charvocab.keys() else charvocab['<oovchar>'] for w in word]\n",
    "        word_charlevel_vocab[vocab[word]] = pad_chars(word_charlevel_vocab[vocab[word]], max_charlen)\n",
    "\n",
    "        wordid2wordlen[vocab[word]] = len(word)\n",
    "        # word_charlevel_vocab[vocab[word]] = word_charlevel_vocab[vocab[word]].extend([charvocab['<padchar>']]*(max_charlen-len(word_charlevel_vocab[vocab[word]])))\n",
    "    return word_charlevel_vocab, wordid2wordlen\n",
    "\n",
    "\n",
    "def load_char_level(X, wordid2word_charlevel_vocab, wordid2wordlen):\n",
    "    #X is of shape (no.of.sentences, 104)\n",
    "    Xcharlevel = [] # will finally be fo shape (total.sentences, max_sent.len, )\n",
    "    Xcharlevel_lengths = []\n",
    "    for i in range(X.shape[0]):\n",
    "        sentence = []\n",
    "        wordlengths = []\n",
    "        for j in range(X.shape[1]):\n",
    "            sentence.append(torch.tensor([wordid2word_charlevel_vocab[int(X[i, j].item())]]))\n",
    "            wordlengths.append(wordid2wordlen[int(X[i, j].item())])\n",
    "            # sentences = pad_sequence(sentences)\n",
    "        # print(i)\n",
    "        Xcharlevel_lengths.append(wordlengths)\n",
    "        Xcharlevel.append(torch.stack(sentence))\n",
    "    \n",
    "    return torch.squeeze(torch.stack(Xcharlevel)), torch.tensor(Xcharlevel_lengths)\n",
    "\n",
    "def get_charvocab(vocab):\n",
    "    # using vocab make charvocab\n",
    "    words = list(vocab.keys())\n",
    "    characters = [char for word in words for char in word]\n",
    "    characters = list(set(characters))\n",
    "    char_vocab = {}\n",
    "    char_vocab[\"<padchar>\"] = 0\n",
    "    char_vocab[\"<oovchar>\"] = 1\n",
    "    for i, char in enumerate(characters):\n",
    "        char_vocab[char] = i+2\n",
    "\n",
    "    return char_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14986, 113]) torch.Size([14986, 113]) torch.Size([14986])\n",
      "torch.Size([3465, 109]) torch.Size([3465, 109]) torch.Size([3465])\n"
     ]
    }
   ],
   "source": [
    "traindatapath = 'data/train.txt'\n",
    "devdatapath = 'data/dev.txt'\n",
    "testdatapath = 'data/test.txt'\n",
    "\n",
    "Xtrain, Ytrain, x_trainlengths, vocab, nertags = load_data(traindatapath, buildvocab_tags=True)\n",
    "Xdev, Ydev, x_devlengths, _, _ = load_data(devdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3465, 109]) torch.Size([3465, 109]) torch.Size([3465])\n"
     ]
    }
   ],
   "source": [
    "# Character Level training data making\n",
    "# make vocabulary of characters from train vocabulary\n",
    "char_vocab = get_charvocab(vocab)\n",
    "wordid2word_charlevel_vocab, wordid2wordlen = make_id2word_charvocab(vocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
    "#make char level train data for the char embeddings \n",
    "Xtrain_char, xlength_char = load_char_level(Xtrain, wordid2word_charlevel_vocab, wordid2wordlen)\n",
    "#finally make the dataloader for train\n",
    "traindataset = TensorDataset(Xtrain, Xtrain_char, Ytrain, x_trainlengths, xlength_char)\n",
    "Trainloader = DataLoader(traindataset, batch_size= 128, shuffle=True)\n",
    "\n",
    "# Character Level validation data making\n",
    "Xdev_temp, Ydev_temp, x_devlengths_temp, devvocab, devnertags = load_data(devdatapath, buildvocab_tags=True)\n",
    "wordid2word_charlevel_vocab_dev, wordid2wordlen_dev = make_id2word_charvocab(devvocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
    "#make char level train data for the char embeddings \n",
    "Xdev_char, xdevlength_char = load_char_level(Xdev_temp, wordid2word_charlevel_vocab_dev, wordid2wordlen_dev)\n",
    "#finally make the dataloader for train\n",
    "devdataset = TensorDataset(Xdev, Xdev_char, Ydev, x_devlengths, xdevlength_char)\n",
    "Devloader = DataLoader(devdataset, batch_size= 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MY MODEL!!! \n",
    "\n",
    "pre_embeddings = 'glove'\n",
    "Expname = 'BILSTM_glove_char'\n",
    "rootpath = 'out/'\n",
    "glove_embeddings_file = 'data/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EMBEDDINGS\n",
    "embedding_size = 100\n",
    "\n",
    "num_words = len(vocab)\n",
    "word_embeds = torch.rand(num_words, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character level onehot embeddings and important classes for performance metrics\n",
    "char_onehot = torch.eye(len(char_vocab))\n",
    "imp_classes = [nertags[tag] for tag in nertags.keys()]\n",
    "imp_classes.remove(nertags['padtag'])\n",
    "imp_classes.remove(nertags['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(embedding_size = 100, hidden_size = 100, total_words = len(vocab), num_class = len(nertags), pretrained = True, pretrained_embed = word_embeds, char_embed_size = len(char_vocab), pretr_char_embed = char_onehot).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3) \n",
    "lossfunction = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (wordembed): Embedding(23626, 100)\n",
      "  (for_charembed): forLSTM(\n",
      "    (charembed): Embedding(88, 88)\n",
      "    (lstm): LSTM(88, 25, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (bilstm): LSTM(150, 100, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(y, ypred, nertags):\n",
    "    y = y.numpy()\n",
    "    ypred = ypred.numpy()\n",
    "    mask = (y != nertags['padtag']) * (y != nertags['O'])\n",
    "    y = y*mask\n",
    "    ypred = ypred*mask\n",
    "    acc = ((y==ypred)*mask).sum()/mask.sum()\n",
    "    microf1 = f1_score(y, ypred, labels = imp_classes, average='micro')\n",
    "    macrof1 = f1_score(y, ypred, labels = imp_classes, average='macro')\n",
    "\n",
    "    return acc, microf1, macrof1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "        with torch.no_grad():\n",
    "            validloss = 0\n",
    "            acc = 0\n",
    "            microf1 = 0\n",
    "            macrof1 = 0\n",
    "            i = 0\n",
    "            for step, (X, Xchar, Y, xlen, xlen_char) in enumerate(loader):\n",
    "                Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
    "                Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
    "                ypred = model(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device))#.permute(0, 2, 1)\n",
    "                vloss = lossfunction(ypred.to('cpu'), Y.view(-1).type(torch.LongTensor))\n",
    "                validloss+=vloss.item()\n",
    "                acc_, microf1_, macrof1_ = performance(Y.view(-1), torch.argmax(ypred.to('cpu'), dim = 1), nertags)\n",
    "                acc+=acc_\n",
    "                microf1 += microf1_\n",
    "                macrof1 += macrof1_\n",
    "                i+=1\n",
    "\n",
    "        return validloss/i, acc/i, microf1/i, macrof1/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlosslist = []\n",
    "trainacclist = [] #accuracy except pad, O\n",
    "trainmicrof1list = []\n",
    "trainmacrof1list = []\n",
    "\n",
    "validlosslist = []\n",
    "valacclist = []\n",
    "valmicrof1list = []\n",
    "valmacrof1list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.0027417027417027417, microF1 = 0.0032312925170068026, macroF1 = 0.00224598021221763\n",
      "training accuracy = 0.0016474002270987825, microF1 = 0.0021338421830090263, macroF1 = 0.0014386705896059207\n",
      "training accuracy = 0.02029210339038567, microF1 = 0.032904532543002976, macroF1 = 0.018036064082439348\n",
      "training accuracy = 0.059521659706339984, microF1 = 0.08535963663914753, macroF1 = 0.04471193914365762\n",
      "training accuracy = 0.10311606815604, microF1 = 0.13696007483454967, macroF1 = 0.07736416024550764\n",
      "\n",
      "epoch = 0, training_loss = 0.1919276345584352, validation_loss =0.10008564657930817, training_acc = 0.14440225015144809, validation_acc =0.41547443926445077\n",
      "training accuracy = 0.4931081116911082, microF1 = 0.5294556467133628, macroF1 = 0.4043848144294892\n",
      "training accuracy = 0.5061359762622567, microF1 = 0.5437206513106206, macroF1 = 0.42210027156909474\n",
      "training accuracy = 0.5359301035504214, microF1 = 0.5738228045368354, macroF1 = 0.46248145412524766\n",
      "training accuracy = 0.5603678451946198, microF1 = 0.5993534445218321, macroF1 = 0.4971612006698365\n",
      "training accuracy = 0.5854833636469696, microF1 = 0.6245181118907072, macroF1 = 0.5325593554420385\n",
      "\n",
      "epoch = 1, training_loss = 0.06472732352440135, validation_loss =0.0654087505037231, training_acc = 0.598898001004881, validation_acc =0.6774292986247732\n",
      "training accuracy = 0.7733268710379281, microF1 = 0.796110307446174, macroF1 = 0.7522485125899122\n",
      "training accuracy = 0.7772427123347146, microF1 = 0.8005061975442765, macroF1 = 0.7620499619045896\n",
      "training accuracy = 0.7881413243241114, microF1 = 0.810907125830269, macroF1 = 0.7728784212661914\n",
      "training accuracy = 0.7964156694334706, microF1 = 0.8188445262902142, macroF1 = 0.7827419037790994\n",
      "training accuracy = 0.803814527997731, microF1 = 0.8259219575810633, macroF1 = 0.7914454134255061\n",
      "\n",
      "epoch = 2, training_loss = 0.033932175203147584, validation_loss =0.05104955072913851, training_acc = 0.8101892099444195, validation_acc =0.7487276610979225\n",
      "training accuracy = 0.8724067009368798, microF1 = 0.8878582256060678, macroF1 = 0.867160265050381\n",
      "training accuracy = 0.874908204221701, microF1 = 0.8906068362779931, macroF1 = 0.8640910307193183\n",
      "training accuracy = 0.8788492296796082, microF1 = 0.8944201506834417, macroF1 = 0.8688601842274394\n",
      "training accuracy = 0.8830391877218797, microF1 = 0.8986181517893063, macroF1 = 0.8721468433132014\n",
      "training accuracy = 0.883256090800056, microF1 = 0.8987464233819904, macroF1 = 0.8712888521449802\n",
      "\n",
      "epoch = 3, training_loss = 0.021129986476468837, validation_loss =0.045056300038205724, training_acc = 0.8844665515476549, validation_acc =0.782854204489455\n",
      "training accuracy = 0.9272131675520463, microF1 = 0.9381031962805325, macroF1 = 0.9184124856454832\n",
      "training accuracy = 0.9215233003398539, microF1 = 0.9332695570103494, macroF1 = 0.9125454622875043\n",
      "training accuracy = 0.922793133758906, microF1 = 0.9340959453370077, macroF1 = 0.9139725891822216\n",
      "training accuracy = 0.923606683628749, microF1 = 0.935324711391298, macroF1 = 0.914601118562925\n",
      "training accuracy = 0.9257626432191114, microF1 = 0.9372744152090884, macroF1 = 0.9173145903298693\n",
      "\n",
      "epoch = 4, training_loss = 0.014347227250809893, validation_loss =0.049997459298798015, training_acc = 0.925596721774184, validation_acc =0.802136380069611\n",
      "training accuracy = 0.9442604773345893, microF1 = 0.9524814700436763, macroF1 = 0.937571652519142\n",
      "training accuracy = 0.9462990414012958, microF1 = 0.9547647319843018, macroF1 = 0.9415909207103814\n",
      "training accuracy = 0.9481202564376285, microF1 = 0.9563157119578142, macroF1 = 0.9440615397977182\n",
      "training accuracy = 0.947472834946958, microF1 = 0.9557443526150147, macroF1 = 0.9428763384671792\n",
      "training accuracy = 0.9482086502768919, microF1 = 0.9562529002330447, macroF1 = 0.943272084595773\n",
      "\n",
      "epoch = 5, training_loss = 0.010552579387075316, validation_loss =0.04552682931534946, training_acc = 0.9464217182452833, validation_acc =0.8133972841624584\n",
      "training accuracy = 0.955398722797649, microF1 = 0.9633414356350752, macroF1 = 0.9483215429577216\n",
      "training accuracy = 0.9583821900101419, microF1 = 0.9654548662033173, macroF1 = 0.9515635917066967\n",
      "training accuracy = 0.9581640445980822, microF1 = 0.9655083820356134, macroF1 = 0.9534854475562474\n",
      "training accuracy = 0.9581302652190841, microF1 = 0.9653052467074619, macroF1 = 0.9522496870888333\n",
      "training accuracy = 0.9579089762488135, microF1 = 0.9650940796109868, macroF1 = 0.9527340641149413\n",
      "\n",
      "epoch = 6, training_loss = 0.00850951024212764, validation_loss =0.04682378497506891, training_acc = 0.9571984164641649, validation_acc =0.8091163001886866\n",
      "training accuracy = 0.9607717384105595, microF1 = 0.9680735845239135, macroF1 = 0.9523347816110682\n",
      "training accuracy = 0.9630083823021821, microF1 = 0.9701843501668947, macroF1 = 0.9573576277011254\n",
      "training accuracy = 0.9644998912340503, microF1 = 0.9712464446392602, macroF1 = 0.9603800573841382\n",
      "training accuracy = 0.9649554973945949, microF1 = 0.9714742853508411, macroF1 = 0.9607017258750745\n",
      "training accuracy = 0.9658383593361893, microF1 = 0.972300623022563, macroF1 = 0.9615358156391016\n",
      "\n",
      "epoch = 7, training_loss = 0.0084909090951827, validation_loss =0.06763613872629191, training_acc = 0.9623460632057356, validation_acc =0.7207059408615432\n",
      "training accuracy = 0.9475382796372042, microF1 = 0.9518463837009025, macroF1 = 0.9255365320535895\n",
      "training accuracy = 0.9582725406579392, microF1 = 0.9634777788225567, macroF1 = 0.9451085002906326\n",
      "training accuracy = 0.9644758870886722, microF1 = 0.9695478841541179, macroF1 = 0.9560725372677807\n",
      "training accuracy = 0.9671687152901373, microF1 = 0.9723059569183954, macroF1 = 0.9609001884137743\n",
      "training accuracy = 0.9688404642949612, microF1 = 0.9738537947350374, macroF1 = 0.9640605278162125\n",
      "\n",
      "epoch = 8, training_loss = 0.005798632197153076, validation_loss =0.050493145428065746, training_acc = 0.969334659738226, validation_acc =0.8072235234341952\n",
      "training accuracy = 0.982042194985665, microF1 = 0.9857084903150695, macroF1 = 0.9818501051232967\n",
      "training accuracy = 0.9796019567148337, microF1 = 0.9835187074834653, macroF1 = 0.9780722677033551\n",
      "training accuracy = 0.9775900746989522, microF1 = 0.9820510227447545, macroF1 = 0.974916120210244\n",
      "training accuracy = 0.9775628198853188, microF1 = 0.9822091143873554, macroF1 = 0.9753745650040772\n",
      "training accuracy = 0.9767081921165632, microF1 = 0.9812974169676851, macroF1 = 0.9743754379035904\n",
      "\n",
      "epoch = 9, training_loss = 0.004447946715626424, validation_loss =0.048404263997716565, training_acc = 0.9769025058416446, validation_acc =0.8172072043424187\n"
     ]
    }
   ],
   "source": [
    "# Model is ready now we have to train using cross entropy loss\n",
    "num_epochs = 10\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# validloss = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    if(epoch == 8):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        \n",
    "    totalloss, acc, microf1, macrof1 = 0, 0, 0, 0\n",
    "    for step, (Xbatch , Xchar ,Ybatch, xbatch_len, xlength_char) in enumerate(Trainloader):\n",
    "        #make gradients 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Ybatch = pack_padded_sequence(Ybatch, xbatch_len, batch_first=True, enforce_sorted=False)\n",
    "        Ybatch, y_lengths = pad_packed_sequence(Ybatch, batch_first=True)\n",
    "\n",
    "        #get output from model and claculate loss\n",
    "        ypred = model(Xbatch.long().to(device), Xchar.to(device), xbatch_len.to(device), xlength_char.to(device))#.permute(0, 2, 1)\n",
    "        acc_, microf1_, macrof1_ = performance(Ybatch.view(-1), torch.argmax(ypred.to('cpu'), dim = 1), nertags)\n",
    "        acc+= acc_\n",
    "        microf1+=microf1_\n",
    "        macrof1+=macrof1_\n",
    "        if(step%20 == 0 and step !=0):\n",
    "            print(\"training accuracy = {}, microF1 = {}, macroF1 = {}\".format(acc/(step+1), microf1/(step+1), macrof1/(step+1)))\n",
    "            \n",
    "        loss = lossfunction(ypred.to('cpu'), Ybatch.view(-1).type(torch.LongTensor)) #Ybatch has dimension (batchsize, seqlen), ypred has dimension(batchsize, num_classes, seqlen)\n",
    "        totalloss += loss.item()\n",
    "\n",
    "        #backward and step\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5) # clip gradient to 5\n",
    "        optimizer.step()\n",
    "        \n",
    "    trainlosslist.append(totalloss/(step+1))\n",
    "    trainacclist.append(acc/(step+1))\n",
    "    trainmicrof1list.append(microf1/(step+1))\n",
    "    trainmacrof1list.append(macrof1/(step+1))\n",
    "\n",
    "    # model validation loss and scheduler step for learning rate change if required\n",
    "    val_loss, val_acc, val_microf1, val_macrof1  = validate(model, Devloader)\n",
    "    validlosslist.append(val_loss)\n",
    "    valacclist.append(val_acc)\n",
    "    valmicrof1list.append(val_microf1)\n",
    "    valmacrof1list.append(val_macrof1)\n",
    "        \n",
    "    # scheduler.step(val_loss)\n",
    "    print('\\nepoch = {}, training_loss = {}, validation_loss ={}, training_acc = {}, validation_acc ={}'.format(epoch, trainlosslist[-1], validlosslist[-1], trainacclist[-1], valacclist[-1]))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {id: word for word, id in vocab.items()} \n",
    "\n",
    "id2tag = {}\n",
    "for tag in nertags.keys():\n",
    "    if(tag == 'padtag'):\n",
    "         id2tag[nertags[tag]] = 'O' # because we dont want the model to predict 'padtag' tags\n",
    "    else:\n",
    "        id2tag[nertags[tag]] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL AND DICTIONARIES ---- delete!!\n",
    "\n",
    "torch.save(model.state_dict(), 'models/bilstm_char/trained_bilstm_model_state_dict.pth')\n",
    "torch.save(model, 'models/bilstm_char/trained_bilstm_model.pth')\n",
    "\n",
    "with open('models/bilstm_char/id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)\n",
    "with open('models/bilstm_char/id2tag.pkl', 'wb') as f:\n",
    "    pickle.dump(id2tag, f)\n",
    "with open('models/bilstm_char/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "with open('models/bilstm_char/nertags.pkl', 'wb') as f:\n",
    "    pickle.dump(nertags, f)\n",
    "with open('models/bilstm_char/char_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(char_vocab, f)\n",
    "with open('models/bilstm_char/char_onehot.pkl', 'wb') as f:\n",
    "    pickle.dump(char_onehot, f)\n",
    "with open('models/bilstm_char/get_charvocab.pkl', 'wb') as f:\n",
    "    pickle.dump(get_charvocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (wordembed): Embedding(23626, 100)\n",
       "  (for_charembed): forLSTM(\n",
       "    (charembed): Embedding(88, 88)\n",
       "    (lstm): LSTM(88, 25, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bilstm): LSTM(150, 100, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=200, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/bilstm_char/trained_bilstm_model_state_dict.pth'))\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (wordembed): Embedding(23626, 100)\n",
       "  (for_charembed): forLSTM(\n",
       "    (charembed): Embedding(88, 88)\n",
       "    (lstm): LSTM(88, 25, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bilstm): LSTM(150, 100, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_metrics(model,loader):\n",
    "    y_predicted = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for step, (X,Xchar, Y, xlen, xlen_char) in enumerate(loader):\n",
    "            Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
    "            Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
    "            ypred = model(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device))#.permute(0, 2, 1)\n",
    "            ypred = torch.argmax(ypred.to('cpu'), dim = 1)\n",
    "            ypred = ypred.view(Y.shape[0], -1)\n",
    "            y_predicted.append(ypred)\n",
    "            y_true.append(Y)\n",
    "\n",
    "    y_predicted_list = []\n",
    "    y_true_list = []\n",
    "    for i in range(len(y_predicted)):\n",
    "        for j in range(y_predicted[i].shape[0]):\n",
    "            sent_pred = []\n",
    "            sent_true = []\n",
    "            for x in range(y_predicted[i].shape[1]):\n",
    "                sent_pred.append(id2tag[int(y_predicted[i][j, x])])\n",
    "                sent_true.append(id2tag[int(y_true[i][j, x])])\n",
    "            y_predicted_list.append(sent_pred)\n",
    "            y_true_list.append(sent_true)\n",
    "    \n",
    "    return seq_f1_score(y_true_list, y_predicted_list), seq_accuracy_score(y_true_list, y_predicted_list), seq_classification_report(y_true_list, y_predicted_list, digits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3683, 124]) torch.Size([3683, 124]) torch.Size([3683])\n",
      "torch.Size([3683, 124]) torch.Size([3683, 124]) torch.Size([3683])\n",
      "PERFORMANCE ON Test DATA\n",
      "MicroF1 = 0.7276526162790697\n",
      "Accuracy = 0.9377477767063109\n",
      "------------Classification Report-------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC      0.897     0.761     0.824      1668\n",
      "        MISC      0.740     0.679     0.708       702\n",
      "         ORG      0.667     0.634     0.650      1661\n",
      "         PER      0.701     0.745     0.722      1617\n",
      "\n",
      "   micro avg      0.747     0.709     0.728      5648\n",
      "   macro avg      0.751     0.705     0.726      5648\n",
      "weighted avg      0.753     0.709     0.729      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test DATASET\n",
    "char_vocab = get_charvocab(vocab)\n",
    "Xtest, Ytest, x_testlengths, _, _ = load_data(testdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)\n",
    "\n",
    "Xtest_temp, Ytest_temp, x_testlengths_temp, testvocab, testnertags = load_data(testdatapath, buildvocab_tags=True)\n",
    "wordid2word_charlevel_vocab_test, wordid2wordlen_test = make_id2word_charvocab(testvocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
    "#make char level train data for the char embeddings \n",
    "Xtest_char, xtestlength_char = load_char_level(Xtest_temp, wordid2word_charlevel_vocab_test, wordid2wordlen_test)\n",
    "#finally make the dataloader for train\n",
    "testdataset = TensorDataset(Xtest, Xtest_char, Ytest, x_testlengths, xtestlength_char)\n",
    "loader_test = DataLoader(testdataset, batch_size= 1, shuffle=False)\n",
    "test_f1_conll, test_acc_conll, test_classif_report = final_metrics(model, loader_test)\n",
    "\n",
    "print(\"PERFORMANCE ON Test DATA\")\n",
    "print('MicroF1 = {}'.format(test_f1_conll))\n",
    "print('Accuracy = {}'.format(test_acc_conll))\n",
    "print('------------Classification Report-------------')\n",
    "print(test_classif_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### deletE?\n",
    "\n",
    "def out_predictions_char(model, loader, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        with torch.no_grad():\n",
    "            for step, (X, Xchar, Y, xlen, xlen_char) in enumerate(loader):\n",
    "                Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
    "                Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
    "                ypred = model(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device))\n",
    "                ypred = torch.argmax(ypred.to('cpu'), dim=1)\n",
    "                ypred = ypred.view(Y.shape[0], -1)\n",
    "                for i in range(len(ypred)):\n",
    "                    for j in range(len(ypred[i])):\n",
    "                        word = id2word[int(X[i, j])]\n",
    "                        tag = id2tag[int(ypred[i, j])]\n",
    "                        f.write(f\"{word}\\t{tag}\\n\")\n",
    "                    f.write('\\n')\n",
    "\n",
    "# Test your character-based model and output predictions to a file\n",
    "out_predictions_char(model, loader_test, 'predictions/gold_bilstm_random.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBIO(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[1])\n",
    "    return ents\n",
    "\n",
    "def toSpans(tags):\n",
    "    spans = set()\n",
    "    for beg in range(len(tags)):\n",
    "        if tags[beg][0] == 'B':\n",
    "            end = beg\n",
    "            for end in range(beg+1, len(tags)):\n",
    "                if tags[end][0] != 'I':\n",
    "                    break\n",
    "            spans.add(str(beg) + '-' + str(end) + ':' + tags[beg][2:])\n",
    "            #print(end-beg)\n",
    "    return spans\n",
    "\n",
    "def getInstanceScores(predPath, goldPath):\n",
    "    goldEnts = readBIO(goldPath)\n",
    "    predEnts = readBIO(predPath)\n",
    "    entScores = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for goldEnt, predEnt in zip(goldEnts, predEnts):\n",
    "        goldSpans = toSpans(goldEnt)\n",
    "        predSpans = toSpans(predEnt)\n",
    "        overlap = len(goldSpans.intersection(predSpans))\n",
    "        tp += overlap\n",
    "        fp += len(predSpans) - overlap\n",
    "        fn += len(goldSpans) - overlap\n",
    "        \n",
    "    prec = 0.0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    rec = 0.0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0.0 if prec+rec == 0.0 else 2 * (prec * rec) / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-F1 score - char-RANDOM- biLSTM  0.7497194163860831\n"
     ]
    }
   ],
   "source": [
    "pred = 'predictions/gold_bilstm_random.txt'\n",
    "gold = 'data/gold.txt'\n",
    "\n",
    "score = getInstanceScores(pred,gold)\n",
    "print('Span-F1 score - char-RANDOM- biLSTM ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL AND DICTIONARIES\n",
    "\n",
    "torch.save(model.state_dict(), 'models/bilstm_char/trained_bilstm_model_state_dict.pth')\n",
    "torch.save(model, 'models/bilstm_char/trained_bilstm_model.pth')\n",
    "\n",
    "with open('models/bilstm_char/id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)\n",
    "with open('models/bilstm_char/id2tag.pkl', 'wb') as f:\n",
    "    pickle.dump(id2tag, f)\n",
    "with open('models/bilstm_char/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "with open('models/bilstm_char/nertags.pkl', 'wb') as f:\n",
    "    pickle.dump(nertags, f)\n",
    "with open('models/bilstm_char/char_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(char_vocab, f)\n",
    "with open('models/bilstm_char/char_onehot.pkl', 'wb') as f:\n",
    "    pickle.dump(char_onehot, f)\n",
    "with open('models/bilstm_char/get_charvocab.pkl', 'wb') as f:\n",
    "    pickle.dump(get_charvocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOISE INJECTION - altered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rates = {\n",
    "    'capitalization_swap': [0.1, 0.15, 0.2, 0.25, 0.3], \n",
    "    'character_swap': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_removal': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'character_replacement': [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_data(datapath, buildvocab_tags= True, vocab = None, nertags = None):\n",
    "    if(buildvocab_tags == True):\n",
    "        all_words = []\n",
    "        all_tags = []\n",
    "        with open(datapath) as f:\n",
    "            lines = f.readlines()\n",
    "            sent_num = 0\n",
    "            for line in lines: #1: so that the first blank line isn't taken into account\n",
    "                if(line == \"\\n\"):\n",
    "                    sent_num+=1\n",
    "                else:\n",
    "                    line_sep = line.split('\\t')\n",
    "                    all_words.append(line_sep[0])\n",
    "                    all_tags.append(line_sep[1][:-1])\n",
    "                    \n",
    "        words = list(set(all_words))\n",
    "        tags = list(set(all_tags))\n",
    "\n",
    "        vocab = {}\n",
    "        vocab['<pad>'] = 0 # for padding input sequences\n",
    "        vocab['<oov>'] = 1\n",
    "        for i, word in enumerate(words):\n",
    "            vocab[word] = i+2\n",
    "            \n",
    "        nertags = {}\n",
    "        nertags['padtag'] = 0\n",
    "        for i,nertag in enumerate(tags):\n",
    "            nertags[nertag] = i+1\n",
    "\n",
    "    train_sent = []\n",
    "    train_tags = []\n",
    "    with open(datapath) as f:\n",
    "        lines = f.readlines()\n",
    "        sent_num = 0\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        for line in lines: #1: so that the first blank line isn't taken into account\n",
    "            if(line == \"\\n\"):\n",
    "                sent_num+=1\n",
    "                train_sent.append(sentence)\n",
    "                train_tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                line_sep = line.split(\"\\t\")\n",
    "                if(line_sep[0] in vocab.keys()):\n",
    "                    sentence.append(vocab[line_sep[0]])\n",
    "                else:\n",
    "                    sentence.append(vocab['<oov>'])  \n",
    "                tag.append(nertags[line_sep[-1][:-1]])\n",
    "\n",
    "    # padding the sentences at the end\n",
    "    seq_maxlen = max(len(x) for x in train_sent)\n",
    "    x_lengths = [len(x) for x in train_sent]\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    for sent, tags in zip(train_sent, train_tags):\n",
    "        length_toappend = seq_maxlen - len(sent)\n",
    "        Xtrain.append(sent+[0]*length_toappend)\n",
    "        Ytrain.append(tags+[0]*length_toappend)\n",
    "\n",
    "\n",
    "    Xtrain = torch.Tensor(Xtrain)\n",
    "    Ytrain = torch.Tensor(Ytrain)\n",
    "    x_lengths = torch.Tensor(x_lengths)\n",
    "    \n",
    "    return Xtrain, Ytrain, x_lengths, vocab, nertags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(testdatapath):\n",
    "    Xtest, Ytest, x_testlengths, _, _ = test_load_data(testdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)\n",
    "\n",
    "    Xtest_temp, Ytest_temp, x_testlengths_temp, testvocab, testnertags = test_load_data(testdatapath, buildvocab_tags=True)\n",
    "    wordid2word_charlevel_vocab_test, wordid2wordlen_test = make_id2word_charvocab(testvocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
    "    #make char level train data for the char embeddings \n",
    "    Xtest_char, xtestlength_char = load_char_level(Xtest_temp, wordid2word_charlevel_vocab_test, wordid2wordlen_test)\n",
    "    #finally make the dataloader for train\n",
    "    testdataset = TensorDataset(Xtest, Xtest_char, Ytest, x_testlengths, xtestlength_char)\n",
    "    loader_test = DataLoader(testdataset, batch_size= 1, shuffle=False)\n",
    "    return loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "    y_predicted = []\n",
    "    with torch.no_grad():\n",
    "        for step, (X, Xchar, Y, xlen, xlen_char) in enumerate(loader):\n",
    "            ypred = model(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device))#.permute(0, 2, 1)\n",
    "            ypred = torch.argmax(ypred.to('cpu'), dim = 1)\n",
    "            ypred = ypred.view(Y.shape[0], -1)\n",
    "            y_predicted.append(ypred)\n",
    "\n",
    "    y_predicted_list = []\n",
    "    for i in range(len(y_predicted)):\n",
    "        for j in range(y_predicted[i].shape[0]):\n",
    "            sent_pred = []\n",
    "            for x in range(y_predicted[i].shape[1]):\n",
    "                sent_pred.append(id2tag[int(y_predicted[i][j, x])])\n",
    "            y_predicted_list.append(sent_pred)\n",
    "    return y_predicted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writefile(testfilepath, outputfilepath, predictions):\n",
    "    final_output = [] #list of lists which will finally be written to file\n",
    "    with open(testfilepath) as f:\n",
    "        lines = f.readlines()\n",
    "        sentnum = 0 #to take care of the first blank line\n",
    "        wordnum = 0\n",
    "        for line in lines:\n",
    "            if(line == '\\n'):\n",
    "                sentnum+=1\n",
    "                wordnum = 0\n",
    "                final_output.append(line)\n",
    "\n",
    "            else:\n",
    "                line_sep = line.split(sep = \"\\t\")\n",
    "                word = line_sep[0]\n",
    "                prediction = predictions[sentnum][wordnum]\n",
    "                final_output.append(f'{word}\\t{prediction}\\n')\n",
    "                wordnum+=1\n",
    "    #write the outputfilepath\n",
    "    with open(outputfilepath, 'w+') as f:\n",
    "        f.writelines(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        path = f'data/altered/{noise_type}_rate_{rate}.txt'\n",
    "        my_loader = loader(path)\n",
    "        predictions = test_model(model, my_loader)\n",
    "        #print(predictions)\n",
    "        writefile(path, f'predictions/altered/random/{noise_type}_rate_{rate}.txt',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Type  Rate  F1 Score\n",
      "0     capitalization_swap  0.10  0.681852\n",
      "1     capitalization_swap  0.15  0.648644\n",
      "2     capitalization_swap  0.20  0.617456\n",
      "3     capitalization_swap  0.25  0.588180\n",
      "4     capitalization_swap  0.30  0.560166\n",
      "5          character_swap  0.10  0.720159\n",
      "6          character_swap  0.15  0.534556\n",
      "7          character_swap  0.20  0.361144\n",
      "8          character_swap  0.25  0.290720\n",
      "9          character_swap  0.30  0.274491\n",
      "10      character_removal  0.10  0.716285\n",
      "11      character_removal  0.15  0.501342\n",
      "12      character_removal  0.20  0.288770\n",
      "13      character_removal  0.25  0.221473\n",
      "14      character_removal  0.30  0.224307\n",
      "15  character_replacement  0.10  0.715989\n",
      "16  character_replacement  0.15  0.498709\n",
      "17  character_replacement  0.20  0.287100\n",
      "18  character_replacement  0.25  0.220120\n",
      "19  character_replacement  0.30  0.219637\n"
     ]
    }
   ],
   "source": [
    "types = []\n",
    "rates = []\n",
    "f1_score = []\n",
    "\n",
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        score = getInstanceScores(f'predictions/altered/random/{noise_type}_rate_{rate}.txt', 'data/gold.txt')\n",
    "        \n",
    "        # Append data to lists\n",
    "        types.append(noise_type)\n",
    "        rates.append(rate)\n",
    "        f1_score.append(score)\n",
    "\n",
    "results = {'Type': types, 'Rate': rates, 'F1 Score': f1_score}\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out/df_altered_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padtag': 0,\n",
       " 'B-LOC': 1,\n",
       " 'I-MISC': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-ORG': 5,\n",
       " 'B-MISC': 6,\n",
       " 'O': 7,\n",
       " 'I-PER': 8,\n",
       " 'I-LOC': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nertags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altered_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rates = {\n",
    "    'capitalization_swap': [0, 0.1, 0.2, 0.3, 0.4],  # Adjust the rates as needed\n",
    "    'character_swap': [0, 0.05, 0.075, 0.1, 0.125, 0.15, 0.17, 0.2],\n",
    "    'character_removal': [0, 0.05, 0.075, 0.1, 0.125, 0.15, 0.17, 0.2],\n",
    "    'character_replacement': [0, 0.05, 0.075, 0.1, 0.125, 0.15, 0.17, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        path = f'data/altered_2/{noise_type}_rate_{rate}.txt'\n",
    "        my_loader = loader(path)\n",
    "        predictions = test_model(model, my_loader)\n",
    "        #print(predictions)\n",
    "        writefile(path, f'predictions/altered_2/char/{noise_type}_rate_{rate}.txt',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Type   Rate  F1 Score\n",
      "0     capitalization_swap  0.000  0.745409\n",
      "1     capitalization_swap  0.100  0.684309\n",
      "2     capitalization_swap  0.200  0.622143\n",
      "3     capitalization_swap  0.300  0.568596\n",
      "4     capitalization_swap  0.400  0.515639\n",
      "5          character_swap  0.000  0.745409\n",
      "6          character_swap  0.050  0.745162\n",
      "7          character_swap  0.075  0.743834\n",
      "8          character_swap  0.100  0.702426\n",
      "9          character_swap  0.125  0.630696\n",
      "10         character_swap  0.150  0.565464\n",
      "11         character_swap  0.170  0.502799\n",
      "12         character_swap  0.200  0.440939\n",
      "13      character_removal  0.000  0.745409\n",
      "14      character_removal  0.050  0.745162\n",
      "15      character_removal  0.075  0.743901\n",
      "16      character_removal  0.100  0.698742\n",
      "17      character_removal  0.125  0.616539\n",
      "18      character_removal  0.150  0.539534\n",
      "19      character_removal  0.170  0.464226\n",
      "20      character_removal  0.200  0.399509\n",
      "21  character_replacement  0.000  0.745409\n",
      "22  character_replacement  0.050  0.745162\n",
      "23  character_replacement  0.075  0.743901\n",
      "24  character_replacement  0.100  0.702158\n",
      "25  character_replacement  0.125  0.617183\n",
      "26  character_replacement  0.150  0.538155\n",
      "27  character_replacement  0.170  0.462769\n",
      "28  character_replacement  0.200  0.392418\n"
     ]
    }
   ],
   "source": [
    "types = []\n",
    "rates = []\n",
    "f1_score = []\n",
    "\n",
    "for noise_type in noise_rates.keys():\n",
    "    for rate in noise_rates[noise_type]:\n",
    "        score = getInstanceScores(f'predictions/altered_2/char/{noise_type}_rate_{rate}.txt', 'data/gold.txt')\n",
    "        \n",
    "        # Append data to lists\n",
    "        types.append(noise_type)\n",
    "        rates.append(rate)\n",
    "        f1_score.append(score)\n",
    "\n",
    "results = {'Type': types, 'Rate': rates, 'F1 Score': f1_score}\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out/df_altered_2_char.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
